{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9566977,"sourceType":"datasetVersion","datasetId":5830902}],"dockerImageVersionId":30350,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Adapted from https://www.kaggle.com/code/taylorsamarel/change-python-version-on-kaggle-taylor-amarel","metadata":{}},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:01:10.447728Z","iopub.execute_input":"2024-11-18T22:01:10.448410Z","iopub.status.idle":"2024-11-18T22:01:11.469294Z","shell.execute_reply.started":"2024-11-18T22:01:10.448325Z","shell.execute_reply":"2024-11-18T22:01:11.468150Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Python 3.7.12\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Create New Conda Environment and Use Conda Channel \n!conda create -n newCondaEnvironment -c cctbx202208 python=3.8.5 -y\n!source /opt/conda/bin/activate newCondaEnvironment && conda install -c cctbx202208 python -y","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:01:11.471643Z","iopub.execute_input":"2024-11-18T22:01:11.472457Z","iopub.status.idle":"2024-11-18T22:05:37.118892Z","shell.execute_reply.started":"2024-11-18T22:01:11.472414Z","shell.execute_reply":"2024-11-18T22:05:37.117981Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 22.9.0\n  latest version: 24.9.2\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda/envs/newCondaEnvironment\n\n  added / updated specs:\n    - python=3.8.5\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    _libgcc_mutex-0.1          |      conda_forge           3 KB  cctbx202208\n    _openmp_mutex-4.5          |            2_gnu          23 KB  cctbx202208\n    ca-certificates-2022.6.15  |       ha878542_0         149 KB  cctbx202208\n    ld_impl_linux-64-2.36.1    |       hea4e1c9_2         667 KB  cctbx202208\n    libffi-3.2.1               |    he1b5a44_1007          47 KB  conda-forge\n    libgcc-ng-12.1.0           |      h8d9b700_16         940 KB  cctbx202208\n    libgomp-12.1.0             |      h8d9b700_16         459 KB  cctbx202208\n    libstdcxx-ng-12.1.0        |      ha89aaad_16         4.3 MB  cctbx202208\n    libzlib-1.2.12             |       h166bdaf_2          63 KB  cctbx202208\n    ncurses-6.3                |       h27087fc_1        1002 KB  cctbx202208\n    openssl-1.1.1q             |       h166bdaf_0         2.1 MB  cctbx202208\n    pip-22.2.2                 |     pyhd8ed1ab_0         1.5 MB  cctbx202208\n    python-3.8.5               |h1103e12_9_cpython        21.9 MB  conda-forge\n    python_abi-3.8             |           2_cp38           4 KB  cctbx202208\n    readline-8.1.2             |       h0f457ee_0         291 KB  cctbx202208\n    setuptools-59.8.0          |   py38h578d9bd_1        1017 KB  cctbx202208\n    sqlite-3.39.2              |       h4ff8645_0         1.5 MB  cctbx202208\n    tk-8.6.12                  |       h27826a3_0         3.3 MB  cctbx202208\n    wheel-0.37.1               |     pyhd8ed1ab_0          31 KB  cctbx202208\n    xz-5.2.5                   |       h516909a_1         343 KB  cctbx202208\n    zlib-1.2.12                |       h166bdaf_2          91 KB  cctbx202208\n    ------------------------------------------------------------\n                                           Total:        39.7 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      cctbx202208/linux-64::_libgcc_mutex-0.1-conda_forge None\n  _openmp_mutex      cctbx202208/linux-64::_openmp_mutex-4.5-2_gnu None\n  ca-certificates    cctbx202208/linux-64::ca-certificates-2022.6.15-ha878542_0 None\n  ld_impl_linux-64   cctbx202208/linux-64::ld_impl_linux-64-2.36.1-hea4e1c9_2 None\n  libffi             conda-forge/linux-64::libffi-3.2.1-he1b5a44_1007 None\n  libgcc-ng          cctbx202208/linux-64::libgcc-ng-12.1.0-h8d9b700_16 None\n  libgomp            cctbx202208/linux-64::libgomp-12.1.0-h8d9b700_16 None\n  libstdcxx-ng       cctbx202208/linux-64::libstdcxx-ng-12.1.0-ha89aaad_16 None\n  libzlib            cctbx202208/linux-64::libzlib-1.2.12-h166bdaf_2 None\n  ncurses            cctbx202208/linux-64::ncurses-6.3-h27087fc_1 None\n  openssl            cctbx202208/linux-64::openssl-1.1.1q-h166bdaf_0 None\n  pip                cctbx202208/noarch::pip-22.2.2-pyhd8ed1ab_0 None\n  python             conda-forge/linux-64::python-3.8.5-h1103e12_9_cpython None\n  python_abi         cctbx202208/linux-64::python_abi-3.8-2_cp38 None\n  readline           cctbx202208/linux-64::readline-8.1.2-h0f457ee_0 None\n  setuptools         cctbx202208/linux-64::setuptools-59.8.0-py38h578d9bd_1 None\n  sqlite             cctbx202208/linux-64::sqlite-3.39.2-h4ff8645_0 None\n  tk                 cctbx202208/linux-64::tk-8.6.12-h27826a3_0 None\n  wheel              cctbx202208/noarch::wheel-0.37.1-pyhd8ed1ab_0 None\n  xz                 cctbx202208/linux-64::xz-5.2.5-h516909a_1 None\n  zlib               cctbx202208/linux-64::zlib-1.2.12-h166bdaf_2 None\n\n\n\nDownloading and Extracting Packages\nxz-5.2.5             | 343 KB    | ##################################### | 100% \nwheel-0.37.1         | 31 KB     | ##################################### | 100% \npip-22.2.2           | 1.5 MB    | ##################################### | 100% \nca-certificates-2022 | 149 KB    | ##################################### | 100% \nsetuptools-59.8.0    | 1017 KB   | ##################################### | 100% \nlibffi-3.2.1         | 47 KB     | ##################################### | 100% \nopenssl-1.1.1q       | 2.1 MB    | ##################################### | 100% \n_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \ntk-8.6.12            | 3.3 MB    | ##################################### | 100% \npython_abi-3.8       | 4 KB      | ##################################### | 100% \n_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \nlibgomp-12.1.0       | 459 KB    | ##################################### | 100% \nlibzlib-1.2.12       | 63 KB     | ##################################### | 100% \nncurses-6.3          | 1002 KB   | ##################################### | 100% \nsqlite-3.39.2        | 1.5 MB    | ##################################### | 100% \npython-3.8.5         | 21.9 MB   | ##################################### | 100% \nreadline-8.1.2       | 291 KB    | ##################################### | 100% \nlibstdcxx-ng-12.1.0  | 4.3 MB    | ##################################### | 100% \nlibgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \nzlib-1.2.12          | 91 KB     | ##################################### | 100% \nld_impl_linux-64-2.3 | 667 KB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate newCondaEnvironment\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 22.9.0\n  latest version: 24.9.2\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\n\n\n# All requested packages already installed.\n\nRetrieving notices: ...working... done\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!/opt/conda/envs/newCondaEnvironment/bin/python3 --version\n!echo 'print(\"Hello, World!\")' > test.py\n!/opt/conda/envs/newCondaEnvironment/bin/python3 test.py","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:37.120166Z","iopub.execute_input":"2024-11-18T22:05:37.120431Z","iopub.status.idle":"2024-11-18T22:05:40.136420Z","shell.execute_reply.started":"2024-11-18T22:05:37.120407Z","shell.execute_reply":"2024-11-18T22:05:40.135275Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Python 3.8.5\nHello, World!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!sudo rm /opt/conda/bin/python3\n!sudo ln -sf /opt/conda/envs/newCondaEnvironment/bin/python3 /opt/conda/bin/python3","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:40.139508Z","iopub.execute_input":"2024-11-18T22:05:40.139925Z","iopub.status.idle":"2024-11-18T22:05:42.178197Z","shell.execute_reply.started":"2024-11-18T22:05:40.139885Z","shell.execute_reply":"2024-11-18T22:05:42.177147Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!sudo rm /opt/conda/bin/python3.7\n!sudo ln -sf /opt/conda/envs/newCondaEnvironment/bin/python3 /opt/conda/bin/python3.7","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:42.179625Z","iopub.execute_input":"2024-11-18T22:05:42.179915Z","iopub.status.idle":"2024-11-18T22:05:44.194775Z","shell.execute_reply.started":"2024-11-18T22:05:42.179888Z","shell.execute_reply":"2024-11-18T22:05:44.193409Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!sudo rm /opt/conda/bin/python\n!sudo ln -s /opt/conda/envs/newCondaEnvironment/bin/python3 /opt/conda/bin/python","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:44.196469Z","iopub.execute_input":"2024-11-18T22:05:44.196883Z","iopub.status.idle":"2024-11-18T22:05:46.219609Z","shell.execute_reply.started":"2024-11-18T22:05:44.196845Z","shell.execute_reply":"2024-11-18T22:05:46.218445Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:46.221300Z","iopub.execute_input":"2024-11-18T22:05:46.221835Z","iopub.status.idle":"2024-11-18T22:05:47.250307Z","shell.execute_reply.started":"2024-11-18T22:05:46.221795Z","shell.execute_reply":"2024-11-18T22:05:47.249120Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Python 3.8.5\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:47.251781Z","iopub.execute_input":"2024-11-18T22:05:47.252092Z","iopub.status.idle":"2024-11-18T22:05:48.257901Z","shell.execute_reply.started":"2024-11-18T22:05:47.252063Z","shell.execute_reply":"2024-11-18T22:05:48.256946Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Python 3.8.5\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install numpy==1.20.1 torch==1.7.1 tqdm==4.56.1 transformers==4.5.1 tensorboard==2.4.1 scikit-learn==0.24.1 PyYAML==5.4.1 click==7.1.2 sentencepiece==0.1.95","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:05:48.259267Z","iopub.execute_input":"2024-11-18T22:05:48.259601Z","iopub.status.idle":"2024-11-18T22:06:44.575414Z","shell.execute_reply.started":"2024-11-18T22:05:48.259569Z","shell.execute_reply":"2024-11-18T22:06:44.574503Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting numpy==1.20.1\n  Downloading numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch==1.7.1\n  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tqdm==4.56.1\n  Downloading tqdm-4.56.1-py2.py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers==4.5.1\n  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorboard==2.4.1\n  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hCollecting scikit-learn==0.24.1\n  Downloading scikit_learn-0.24.1-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting PyYAML==5.4.1\n  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting click==7.1.2\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentencepiece==0.1.95\n  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-extensions\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting regex!=2019.12.17\n  Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting filelock\n  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting packaging\n  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protobuf>=3.6.0\n  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting werkzeug>=0.11.15\n  Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/envs/newCondaEnvironment/lib/python3.8/site-packages (from tensorboard==2.4.1) (0.37.1)\nCollecting google-auth<2,>=1.6.3\n  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting grpcio>=1.24.3\n  Downloading grpcio-1.68.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/newCondaEnvironment/lib/python3.8/site-packages (from tensorboard==2.4.1) (59.8.0)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting absl-py>=0.4\n  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting six>=1.10.0\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting markdown>=2.6.8\n  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting scipy>=0.19.1\n  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\nCollecting joblib>=0.11\n  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\nCollecting importlib-metadata>=4.4\n  Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting charset-normalizer<4,>=2\n  Downloading charset_normalizer-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting certifi>=2017.4.17\n  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting MarkupSafe>=2.1.1\n  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\nCollecting zipp>=3.20\n  Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\nCollecting pyasn1<0.7.0,>=0.4.6\n  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, tensorboard-plugin-wit, sentencepiece, zipp, urllib3, typing-extensions, tqdm, threadpoolctl, six, regex, PyYAML, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, joblib, idna, grpcio, filelock, click, charset-normalizer, certifi, cachetools, absl-py, werkzeug, torch, scipy, sacremoses, rsa, requests, pyasn1-modules, importlib-metadata, transformers, scikit-learn, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\n\u001b[33m  WARNING: The script tqdm is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script sacremoses is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script markdown_py is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/opt/conda/envs/newCondaEnvironment/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 PyYAML-5.4.1 absl-py-2.1.0 cachetools-4.2.4 certifi-2024.8.30 charset-normalizer-3.4.0 click-7.1.2 filelock-3.16.1 google-auth-1.35.0 google-auth-oauthlib-0.4.6 grpcio-1.68.0 idna-3.10 importlib-metadata-8.5.0 joblib-1.4.2 markdown-3.7 numpy-1.20.1 oauthlib-3.2.2 packaging-24.2 protobuf-5.28.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 regex-2024.11.6 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 sacremoses-0.1.1 scikit-learn-0.24.1 scipy-1.10.1 sentencepiece-0.1.95 six-1.16.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.1 threadpoolctl-3.5.0 tokenizers-0.10.3 torch-1.7.1 tqdm-4.56.1 transformers-4.5.1 typing-extensions-4.12.2 urllib3-2.2.3 werkzeug-3.0.6 zipp-3.20.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import json\nimport logging\nimport os\n\nimport click\nimport torch\nimport transformers\nfrom transformers import AutoConfig, AutoTokenizer\nfrom transformers.trainer_utils import is_main_process","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:44.767265Z","iopub.execute_input":"2024-11-18T22:13:44.768171Z","iopub.status.idle":"2024-11-18T22:13:44.772708Z","shell.execute_reply.started":"2024-11-18T22:13:44.768135Z","shell.execute_reply":"2024-11-18T22:13:44.771771Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import yaml\ntry:\n    from yaml import CLoader as Loader\nexcept ImportError:\n    from yaml import Loader\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.249226Z","iopub.execute_input":"2024-11-18T22:13:45.249841Z","iopub.status.idle":"2024-11-18T22:13:45.254167Z","shell.execute_reply.started":"2024-11-18T22:13:45.249803Z","shell.execute_reply":"2024-11-18T22:13:45.253123Z"},"trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def load_conf(path):\n    with open(path) as fin:\n        conf_txt = fin.read()\n    conf = yaml.load(conf_txt, Loader=Loader)\n    assert 'raw_yaml' not in conf\n    conf['raw_yaml'] = conf_txt\n\n    if conf['task'] not in ['identification_classification', 'classification']:\n        raise ValueError(\n            \"task must be either 'classification' or 'identification_classification'\")\n\n    if conf['task'] == 'identification_classification' and conf['doc_stride'] >= conf['max_seq_length'] - conf['max_query_length']:\n        raise RuntimeError(\n            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n            \"stride or increase the maximum length to ensure the features are correctly built.\"\n        )\n    return conf\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.288039Z","iopub.execute_input":"2024-11-18T22:13:45.288290Z","iopub.status.idle":"2024-11-18T22:13:45.294168Z","shell.execute_reply.started":"2024-11-18T22:13:45.288267Z","shell.execute_reply":"2024-11-18T22:13:45.293269Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from typing import Tuple, List, Optional, Union\nimport torch\nfrom torch.utils.data import TensorDataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.327452Z","iopub.execute_input":"2024-11-18T22:13:45.327965Z","iopub.status.idle":"2024-11-18T22:13:45.331691Z","shell.execute_reply.started":"2024-11-18T22:13:45.327940Z","shell.execute_reply":"2024-11-18T22:13:45.330809Z"},"trusted":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Tuple\nimport enum\n\n# import tqdm\n\n\nclass NLILabel(enum.Enum):\n    NOT_MENTIONED = 0\n    ENTAILMENT = 1\n    CONTRADICTION = 2\n\n    @classmethod\n    def from_str(cls, s: str):\n        if s == 'NotMentioned':\n            return cls.NOT_MENTIONED\n        elif s == 'Entailment':\n            return cls.ENTAILMENT\n        elif s == 'Contradiction':\n            return cls.CONTRADICTION\n        else:\n            raise ValueError(f'Invalid input \"{\"s\"}\" to NLILabel.from_str.')\n\n    def to_anno_name(self):\n        if self == NLILabel.NOT_MENTIONED:\n            return 'NotMentioned'\n        elif self == NLILabel.ENTAILMENT:\n            return 'Entailment'\n        elif self == NLILabel.CONTRADICTION:\n            return 'Contradiction'\n        else:\n            assert not 'Should not get here'\n\n\nclass ContractNLIExample:\n    \"\"\"\n    A single training/test example for the contract NLI.\n\n    Args:\n        data_id: The example's unique identifier\n        hypothesis_text: The hypothesis string\n        context_text: The context string\n        answer_text: The answer string\n        start_position_character: The character position of the start of the answer\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        data_id,\n        document_id,\n        hypothesis_id,\n        file_name,\n        hypothesis_text,\n        hypothesis_tokens,\n        context_text,\n        tokens,\n        splits,\n        spans,\n        char_to_word_offset,\n        label,\n        annotated_spans\n    ):\n        self.data_id: str = data_id\n        self.document_id: str = document_id\n        self.hypothesis_id: str = hypothesis_id\n        self.hypothesis_symbol: str = f'[{hypothesis_id}]'\n        self.file_name: str = file_name\n        self.hypothesis_text: str = hypothesis_text\n        self.hypothesis_tokens: List[str] = hypothesis_tokens\n        self.context_text: str = context_text\n        self.tokens: List[str] = tokens\n        # Note that splits are NOT unique\n        self.splits: List[int] = splits\n        self.spans: List[Tuple[int, int]] = spans\n        self.char_to_word_offset: List[int] = char_to_word_offset\n        self.label: NLILabel = label\n        self.annotated_spans: List[int] = annotated_spans\n\n    @staticmethod\n    def tokenize_and_align(text: str, spans: List[Tuple[int, int]]):\n        \"\"\"\n        spans: Spans as character offsets. e.g. \"world\" in \"Hello, world\" will\n            be represented as (7, 12).\n        \"\"\"\n        # Split on whitespace so that different tokens may be attributed to their original position.\n        tokens = []\n        char_to_word_offset = []\n        prev_is_whitespace = True\n        splits = {si for s in spans for si in s}\n\n        for i, c in enumerate(text):\n            if c == ' ':\n                # splits will be ignored on space\n                prev_is_whitespace = True\n            else:\n                if prev_is_whitespace or i in splits:\n                    tokens.append(c)\n                else:\n                    tokens[-1] += c\n                prev_is_whitespace = False\n            # len(tokens) == 0 when first characters are spaces\n            char_to_word_offset.append(max(len(tokens) - 1, 0))\n\n        splits = [char_to_word_offset[s[0]] for s in spans]\n        return tokens, splits, char_to_word_offset\n\n    @classmethod\n    def load(cls, input_data) -> List['ContractNLIExample']:\n        examples = []\n        label_dict = {\n            label_id: label_info['hypothesis']\n            for label_id, label_info in input_data['labels'].items()}\n        for document in tqdm(input_data['documents']):\n            if len(document['annotation_sets']) != 1:\n                raise RuntimeError(\n                    f'{len(document[\"annotation_sets\"])} annotation sets given but '\n                    'we only support single annotation set.')\n            for label_id, annotation in document['annotation_sets'][0]['annotations'].items():\n                data_id = f'{document[\"id\"]}_{label_id}'\n                context_text = document['text']\n                hypothesis_text = label_dict[label_id]\n\n                tokens, splits, char_to_word_offset = cls.tokenize_and_align(\n                    context_text, document['spans'])\n                hypothesis_tokens, _, _ = cls.tokenize_and_align(\n                    hypothesis_text, [])\n                assert len(splits) == len(document['spans'])\n                example = cls(\n                    data_id=data_id,\n                    document_id=document[\"id\"],\n                    hypothesis_id=label_id,\n                    file_name=document['file_name'],\n                    hypothesis_text=hypothesis_text,\n                    hypothesis_tokens=hypothesis_tokens,\n                    context_text=context_text,\n                    tokens=tokens,\n                    splits=splits,\n                    spans=document['spans'],\n                    char_to_word_offset=char_to_word_offset,\n                    label=NLILabel.from_str(annotation['choice']),\n                    annotated_spans=annotation['spans']\n                )\n                examples.append(example)\n        return examples\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.370118Z","iopub.execute_input":"2024-11-18T22:13:45.370759Z","iopub.status.idle":"2024-11-18T22:13:45.395354Z","shell.execute_reply.started":"2024-11-18T22:13:45.370728Z","shell.execute_reply":"2024-11-18T22:13:45.394382Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Copyright 2020 The HuggingFace Team. and Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/495c157d6fcfa29f2d9e1173582d2fb5a393c323/src/transformers/data/processors/squad.py\n# and has been modified. See git log for the full details of changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom functools import partial\nfrom multiprocessing import Pool, cpu_count\nfrom typing import List, Dict\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom transformers.tokenization_utils_base import BatchEncoding, \\\n    PreTrainedTokenizerBase\nfrom transformers.utils import logging\n\n# from contract_nli.dataset.loader import ContractNLIExample, NLILabel\n\n# Store the tokenizers which insert 2 separators tokens\nMULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\nSPAN_TOKEN = '[SPAN]'\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass IdentificationClassificationFeatures:\n    \"\"\"\n    Single example features to be fed to a model. Those features are model-specific and can be crafted from\n    :class:`~contract_nli.dataset.loader.ContractNLIExample` using the\n    :method:`~contract_nli.dataset.encoder.convert_examples_to_features` method.\n\n    Args:\n        input_ids: Indices of input sequence tokens in the vocabulary.\n        attention_mask: Mask to avoid performing attention on padding token indices.\n        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n        cls_index: the index of the CLS token.\n        p_mask: Mask identifying tokens that can be answers vs. tokens that cannot.\n            Mask with 1 for tokens than cannot be in the answer and 0 for token that can be in an answer\n        example_index: the index of the example\n        unique_id: The unique Feature identifier\n        paragraph_len: The length of the context\n        token_is_max_context: List of booleans identifying which tokens have their maximum context in this feature object.\n            If a token does not have their maximum context in this feature object, it means that another feature object\n            has more information related to that token and should be prioritized over this feature for that token.\n        tokens: list of tokens corresponding to the input ids\n        token_to_orig_map: mapping between the tokens and the original text.\n        span_to_orig_map: mapping between the spans and the original spans, needed in order to identify the answer.\n        class_label:\n        span_labels:\n        valid_span_missing_in_context: Class label is NOT \"not mentioned\" and a valid span is not in the context\n        data_id:\n        encoding: optionally store the BatchEncoding with the fast-tokenizer alignement methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        cls_index,\n        p_mask,\n        example_index,\n        unique_id,\n        paragraph_len,\n        token_is_max_context,\n        tokens,\n        token_to_orig_map,\n        span_to_orig_map,\n        class_label,\n        span_labels,\n        valid_span_missing_in_context,\n        data_id: str = None,\n        encoding: BatchEncoding = None,\n    ):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.token_type_ids = token_type_ids\n        self.cls_index = cls_index\n        self.p_mask = p_mask\n\n        self.example_index = example_index\n        self.unique_id = unique_id\n        self.paragraph_len = paragraph_len\n        self.token_is_max_context = token_is_max_context\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.span_to_orig_map: Dict[int, List[int]] = span_to_orig_map\n\n        self.class_label = class_label\n        self.span_labels = span_labels\n        if valid_span_missing_in_context:\n            assert class_label in [NLILabel.ENTAILMENT.value, NLILabel.CONTRADICTION.value]\n        self.valid_span_missing_in_context = valid_span_missing_in_context\n        self.data_id = data_id\n\n        self.encoding = encoding\n\n\ndef _new_check_is_max_context(doc_spans, cur_span_index, position):\n    \"\"\"Check if this is the 'max context' doc span for the token at a position\n    `position`.\n    \"\"\"\n    # if len(doc_spans) == 1:\n    # return True\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span[\"start\"] + doc_span[\"paragraph_len\"] - 1\n        if position < doc_span[\"start\"]:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span[\"start\"]\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"paragraph_len\"]\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\ndef tokenize(tokenizer, tokens: List[str], splits: List[int]):\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n    tok_to_orig_span_index = defaultdict(list)\n    for i, s in enumerate(splits):\n        tok_to_orig_span_index[s].append(i)\n    span_to_orig_index = dict()\n    for (i, token) in enumerate(tokens):\n        if i in tok_to_orig_span_index:\n            span_to_orig_index[len(all_doc_tokens)] = tok_to_orig_span_index[i]\n            tok_to_orig_index.append(-1)\n            all_doc_tokens.append(SPAN_TOKEN)\n        orig_to_tok_index.append(len(all_doc_tokens))\n        if tokenizer.__class__.__name__ in [\n            \"RobertaTokenizer\",\n            \"LongformerTokenizer\",\n            \"BartTokenizer\",\n            \"RobertaTokenizerFast\",\n            \"LongformerTokenizerFast\",\n            \"BartTokenizerFast\",\n        ]:\n            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)\n        else:\n            sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_doc_tokens.append(sub_token)\n    return all_doc_tokens, orig_to_tok_index, tok_to_orig_index, span_to_orig_index\n\n\ndef convert_example_to_features(\n        example: ContractNLIExample,\n        max_seq_length: int,\n        doc_stride: int,\n        max_query_length: int,\n        padding_strategy,\n        labels_available: bool,\n        symbol_based_hypothesis: bool\n        ) -> List[IdentificationClassificationFeatures]:\n    features = []\n\n    all_doc_tokens, orig_to_tok_index, tok_to_orig_index, span_to_orig_index = tokenize(\n        tokenizer, example.tokens, example.splits)\n\n    if symbol_based_hypothesis:\n        truncated_query = [example.hypothesis_symbol]\n    else:\n        truncated_query = tokenize(tokenizer, example.hypothesis_tokens, [])[0][:max_query_length]\n\n    # Tokenizers who insert 2 SEP tokens in-between <context> & <question> need to have special handling\n    # in the way they compute mask of added tokens.\n    tokenizer_type = type(tokenizer).__name__.replace(\"Tokenizer\", \"\").lower()\n    sequence_added_tokens = (\n        tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1\n        if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET\n        else tokenizer.model_max_length - tokenizer.max_len_single_sentence\n    )\n    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair\n    query_with_special_tokens_length = len(truncated_query) + sequence_added_tokens\n    max_context_length = max_seq_length - sequence_pair_added_tokens - len(truncated_query)\n\n    spans = []\n    start = 0\n    covered_splits = set()\n    all_splits = set(span_to_orig_index.keys())\n    while len(all_splits - covered_splits) > 0:\n        upcoming_splits = [i for i in span_to_orig_index.keys()\n                           if i >= start and i not in covered_splits]\n        assert len(upcoming_splits) > 0\n        second_split = upcoming_splits[1] if len(upcoming_splits) > 1 else len(all_doc_tokens)\n        if second_split - upcoming_splits[0] > max_context_length:\n            # a single span is larger than maximum allowed tokens ---- there are nothing we can do\n            start = upcoming_splits[0]\n            last_span_idx = second_split\n            covered_splits.add(upcoming_splits[0])\n        elif second_split - start > max_context_length:\n            # we can fit the first upcoming span if we modify \"start\"\n            start += (second_split - max_context_length)\n            last_span_idx = second_split\n            covered_splits.add(upcoming_splits[0])\n        else:\n            # we can fit at least one span\n            last_span_idx = None\n            for i in range(start, min(start + max_context_length, len(all_doc_tokens)) + 1):\n                if i == len(all_doc_tokens) or i in span_to_orig_index:\n                    if last_span_idx is not None:\n                        covered_splits.add(last_span_idx)\n                    last_span_idx = i\n            assert last_span_idx is not None\n\n        split_tokens = all_doc_tokens[start:min(start + max_context_length, len(all_doc_tokens))]\n\n        # Define the side we want to truncate / pad and the text/pair sorting\n        if tokenizer.padding_side == \"right\":\n            texts = truncated_query\n            pairs = split_tokens\n        else:\n            texts = split_tokens\n            pairs = truncated_query\n\n        encoded_dict = tokenizer.encode_plus(\n            texts,\n            pairs,\n            truncation=False,\n            padding=padding_strategy,\n            max_length=max_seq_length,\n            return_overflowing_tokens=False,\n            return_token_type_ids=True\n        )\n        assert len(encoded_dict['input_ids']) <= max_seq_length\n\n        paragraph_len = len(split_tokens)\n        if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n            if tokenizer.padding_side == \"right\":\n                non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n            else:\n                last_padding_id_position = (\n                    len(encoded_dict[\"input_ids\"]) - 1 - encoded_dict[\"input_ids\"][::-1].index(tokenizer.pad_token_id)\n                )\n                non_padded_ids = encoded_dict[\"input_ids\"][last_padding_id_position + 1 :]\n        else:\n            non_padded_ids = encoded_dict[\"input_ids\"]\n\n        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n\n        token_to_orig_map = {}\n        span_to_orig_map = {}\n        for i in range(paragraph_len):\n            index = query_with_special_tokens_length + i if tokenizer.padding_side == \"right\" else i\n            if tok_to_orig_index[start + i] != -1:\n                token_to_orig_map[index] = tok_to_orig_index[start + i]\n                assert (start + i) not in span_to_orig_index\n            else:\n                assert (start + i) in span_to_orig_index\n                span_to_orig_map[index] = span_to_orig_index[start + i]\n\n        encoded_dict[\"paragraph_len\"] = paragraph_len\n        encoded_dict[\"tokens\"] = tokens\n        encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n        encoded_dict[\"span_to_orig_map\"] = span_to_orig_map\n        encoded_dict[\"truncated_query_with_special_tokens_length\"] = query_with_special_tokens_length\n        encoded_dict[\"token_is_max_context\"] = {}\n        encoded_dict[\"start\"] = start\n\n        spans.append(encoded_dict)\n\n        start = last_span_idx - doc_stride\n\n    # Due to striding splitting, the same token will appear multiple times\n    # in different splits. We annotate data with \"token_is_max_context\"\n    # which classifies whether an instance of a token has the longest context\n    # amongst different instances of the same token.\n    for doc_span_index in range(len(spans)):\n        for j in range(spans[doc_span_index][\"paragraph_len\"]):\n            is_max_context = _new_check_is_max_context(\n                spans, doc_span_index, spans[doc_span_index]['start'] + j)\n            index = (\n                j\n                if tokenizer.padding_side == \"left\"\n                else query_with_special_tokens_length + j\n            )\n            spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n\n    span_token_id = tokenizer.additional_special_tokens_ids[tokenizer.additional_special_tokens.index(SPAN_TOKEN)]\n    for span in spans:\n        # Identify the position of the CLS token\n        cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n\n        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n        p_mask = np.logical_not(\n            np.isin(np.array(span[\"input_ids\"]), [span_token_id, tokenizer.cls_token_id])\n        ).astype(np.int32)\n\n        valid_span_missing_in_context = False\n        span_labels = np.zeros_like(span[\"input_ids\"])\n        if labels_available:\n            if example.label != NLILabel.NOT_MENTIONED:\n                doc_start = span[\"start\"]\n                doc_end = span[\"start\"] + span[\"paragraph_len\"]\n                annotated_spans = set(example.annotated_spans)\n                _span_labels = np.array([\n                    any((s in annotated_spans for s in span_to_orig_index.get(i, [])))\n                    for i in range(doc_start, doc_end)\n                ]).astype(int)\n                if not np.any(_span_labels):\n                    valid_span_missing_in_context = True\n                tok_start = query_with_special_tokens_length\n                tok_end = tok_start + span[\"paragraph_len\"]\n                if tokenizer.padding_side == \"right\":\n                    span_labels[tok_start:tok_end] = _span_labels\n                else:\n                    span_labels[-tok_end:-tok_start] = _span_labels\n            class_label = example.label.value\n        else:\n            class_label = -1\n\n        assert not np.any(np.logical_and(p_mask, span_labels))\n\n        features.append(\n            IdentificationClassificationFeatures(\n                span[\"input_ids\"],\n                span[\"attention_mask\"],\n                span[\"token_type_ids\"],\n                cls_index,\n                p_mask,\n                example_index=0,  # Can not set unique_id and example_index here. They will be set after multiple processing.\n                unique_id=0,\n                paragraph_len=span[\"paragraph_len\"],\n                token_is_max_context=span[\"token_is_max_context\"],\n                tokens=span[\"tokens\"],\n                token_to_orig_map=span[\"token_to_orig_map\"],\n                span_to_orig_map=span[\"span_to_orig_map\"],\n                class_label=class_label,\n                span_labels=span_labels,\n                valid_span_missing_in_context=valid_span_missing_in_context,\n                data_id=example.data_id,\n            )\n        )\n    return features\n\n\n\ndef convert_example_to_features_init(tokenizer_for_convert: PreTrainedTokenizerBase):\n    global tokenizer\n    tokenizer = tokenizer_for_convert\n\n\ndef convert_examples_to_features(\n    examples,\n    tokenizer,\n    max_seq_length,\n    doc_stride,\n    max_query_length,\n    labels_available,\n    symbol_based_hypothesis: bool,\n    padding_strategy=\"max_length\",\n    threads=None,\n    tqdm_enabled=True,\n):\n    \"\"\"\n    Converts a list of examples into a list of features that can be directly\n    given as input to a model.\n\n    Args:\n        examples: list of :class:`~contract_nli.dataset.loader.ContractNLIExample`\n        tokenizer: an instance of a child of :class:`~transformers.PreTrainedTokenizer`\n        max_seq_length: The maximum sequence length of the inputs.\n        doc_stride: The stride used when the context is too large and is split across several features.\n        max_query_length: The maximum length of the query.\n        labels_available: whether to create features for model evaluation or model training.\n        padding_strategy: Default to \"max_length\". Which padding strategy to use\n        threads: multiple processing threads.\n    \"\"\"\n    if threads is None or threads < 0:\n        threads = cpu_count()\n    else:\n        threads = min(threads, cpu_count())\n    with Pool(threads, initializer=convert_example_to_features_init, initargs=(tokenizer,)) as p:\n        annotate_ = partial(\n            convert_example_to_features,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            padding_strategy=padding_strategy,\n            labels_available=labels_available,\n            symbol_based_hypothesis=symbol_based_hypothesis\n        )\n        features: List[List[IdentificationClassificationFeatures]] = list(\n            tqdm(\n                p.imap(annotate_, examples, chunksize=32),\n                total=len(examples),\n                desc=\"convert examples to features\",\n                disable=not tqdm_enabled,\n            )\n        )\n    new_features = []\n    unique_id = 1000000000\n    example_index = 0\n    for example_features in tqdm(\n        features, total=len(features), desc=\"add example index and unique id\", disable=not tqdm_enabled\n    ):\n        if not example_features:\n            continue\n        for example_feature in example_features:\n            example_feature.example_index = example_index\n            example_feature.unique_id = unique_id\n            new_features.append(example_feature)\n            unique_id += 1\n        example_index += 1\n    features: List[IdentificationClassificationFeatures] = new_features\n    del new_features\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n    all_valid_span_missing_in_context = torch.tensor([f.valid_span_missing_in_context for f in features], dtype=torch.float)\n\n    all_feature_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n    dataset = [\n        all_input_ids,\n        all_attention_masks,\n        all_token_type_ids,\n        all_cls_index,\n        all_p_mask,\n        all_valid_span_missing_in_context,\n        all_feature_index\n    ]\n    if labels_available:\n        all_class_label = torch.tensor(\n            [f.class_label for f in features], dtype=torch.long)\n        all_span_labels = torch.tensor(\n            [f.span_labels for f in features], dtype=torch.long)\n        dataset += [\n            all_class_label,\n            all_span_labels,\n        ]\n    dataset = TensorDataset(*dataset)\n    return features, dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.578400Z","iopub.execute_input":"2024-11-18T22:13:45.578763Z","iopub.status.idle":"2024-11-18T22:13:45.659755Z","shell.execute_reply.started":"2024-11-18T22:13:45.578732Z","shell.execute_reply":"2024-11-18T22:13:45.658685Z"},"trusted":true},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Copyright 2020 The HuggingFace Team. and Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/495c157d6fcfa29f2d9e1173582d2fb5a393c323/src/transformers/data/processors/squad.py\n# and has been modified. See git log for the full details of changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom functools import partial\nfrom multiprocessing import Pool, cpu_count\nfrom typing import List\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.utils import logging\n\n# from contract_nli.dataset.encoder import tokenize, SPAN_TOKEN\n# from contract_nli.dataset.loader import ContractNLIExample, NLILabel\n\nlogger = logging.get_logger(__name__)\n\n\nclass ClassificationFeatures:\n    \"\"\"\n    Single example features to be fed to a model.\n\n    Args:\n        input_ids: Indices of input sequence tokens in the vocabulary.\n        attention_mask: Mask to avoid performing attention on padding token indices.\n        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n        cls_index: the index of the CLS token.\n        p_mask: Mask identifying tokens that can be answers or not.\n            Mask with 1 for tokens than cannot be in the answer and 0 for token that can be in an answer\n        example_index: the index of the example\n        unique_id: The unique Feature identifier\n        tokens: list of tokens corresponding to the input ids\n        class_label:\n        data_id:\n    \"\"\"\n    def __init__(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        cls_index,\n        p_mask,\n        example_index,\n        unique_id,\n        tokens,\n        class_label,\n        data_id: str = None\n    ):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.token_type_ids = token_type_ids\n        self.cls_index = cls_index\n        self.p_mask = p_mask\n\n        self.example_index = example_index\n        self.unique_id = unique_id\n        self.tokens = tokens\n\n        self.class_label = class_label\n        self.data_id = data_id\n\n        \n## yaha pe hum aa gae hai\n\ndef convert_examples_to_classification_features(\n        example: ContractNLIExample,\n        max_seq_length: int,\n        max_query_length: int,\n        padding_strategy,\n        symbol_based_hypothesis: bool\n        ) -> ClassificationFeatures:\n    all_doc_tokens, orig_to_tok_index, tok_to_orig_index, span_to_orig_index = tokenize(\n       tokenizer, example.tokens, example.splits)\n\n    relevant_tokens = []\n    for s in example.annotated_spans:\n        start_token_index = orig_to_tok_index[example.splits[s]]\n        if s + 1 < len(example.splits):\n            end_token_index = orig_to_tok_index[example.splits[s + 1]] - 1\n        else:\n            end_token_index = orig_to_tok_index[-1]\n        relevant_tokens.extend(all_doc_tokens[start_token_index:end_token_index])\n    assert len(relevant_tokens) > 0 and SPAN_TOKEN not in relevant_tokens\n\n    if symbol_based_hypothesis:\n        truncated_query = [example.hypothesis_symbol]\n    else:\n        truncated_query = tokenize(\n            tokenizer, example.hypothesis_tokens, [])[0][:max_query_length]\n\n    # Define the side we want to truncate / pad and the text/pair sorting\n    if tokenizer.padding_side == \"right\":\n        texts = truncated_query\n        pairs = relevant_tokens\n        truncation = 'only_second'\n    else:\n        texts = relevant_tokens\n        pairs = truncated_query\n        truncation = 'only_first'\n\n    encoded_dict = tokenizer.encode_plus(\n        texts,\n        pairs,\n        truncation=truncation,\n        padding=padding_strategy,\n        max_length=max_seq_length,\n        return_overflowing_tokens=False,\n        return_token_type_ids=True\n    )\n    assert len(encoded_dict['input_ids']) <= max_seq_length\n\n    if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n        if tokenizer.padding_side == \"right\":\n            non_padded_ids = encoded_dict[\"input_ids\"][:encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n        else:\n            last_padding_id_position = (\n                len(encoded_dict[\"input_ids\"]) - 1 - encoded_dict[\"input_ids\"][::-1].index(tokenizer.pad_token_id)\n            )\n            non_padded_ids = encoded_dict[\"input_ids\"][last_padding_id_position + 1:]\n    else:\n        non_padded_ids = encoded_dict[\"input_ids\"]\n\n    tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n\n    # Identify the position of the CLS token\n    cls_index = encoded_dict[\"input_ids\"].index(tokenizer.cls_token_id)\n\n    # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n    p_mask = (np.array(encoded_dict[\"input_ids\"]) != tokenizer.cls_token_id).astype(np.int32)\n\n    return ClassificationFeatures(\n        encoded_dict[\"input_ids\"],\n        encoded_dict[\"attention_mask\"],\n        encoded_dict[\"token_type_ids\"],\n        cls_index,\n        p_mask,\n        example_index=0,  # Can not set unique_id and example_index here. They will be set after multiple processing.\n        unique_id=0,\n        tokens=tokens,\n        class_label=example.label.value,\n        data_id=example.data_id,\n    )\n\n\ndef convert_example_to_features_init(tokenizer_for_convert: PreTrainedTokenizerBase):\n    global tokenizer\n    tokenizer = tokenizer_for_convert\n\n\ndef convert_examples_to_classification_features(\n    examples: List[ContractNLIExample],\n    tokenizer,\n    max_seq_length,\n    max_query_length,\n    symbol_based_hypothesis: bool,\n    padding_strategy=\"max_length\",\n    threads=None,\n    tqdm_enabled=True,\n):\n    \"\"\"\n    Converts a list of examples into a list of features that can be directly\n    given as input to a model.\n\n    Args:\n        examples: list of :class:`~contract_nli.dataset.loader.ContractNLIExample`\n        tokenizer: an instance of a child of :class:`~transformers.PreTrainedTokenizer`\n        max_seq_length: The maximum sequence length of the inputs.\n        doc_stride: The stride used when the context is too large and is split across several features.\n        max_query_length: The maximum length of the query.\n        labels_available: whether to create features for model evaluation or model training.\n        padding_strategy: Default to \"max_length\". Which padding strategy to use\n        threads: multiple processing threads.\n    \"\"\"\n    if threads is None or threads < 0:\n        threads = cpu_count()\n    else:\n        threads = min(threads, cpu_count())\n    n_orig_examples = len(examples)\n    examples = [e for e in examples if e.label != NLILabel.NOT_MENTIONED]\n    logger.warning(\n        f'Removed examples with \"na\" labels ({n_orig_examples} -> {len(examples)})')\n    with Pool(threads, initializer=convert_example_to_features_init, initargs=(tokenizer,)) as p:\n        annotate_ = partial(\n            convert_example_to_features,\n            max_seq_length=max_seq_length,\n            max_query_length=max_query_length,\n            padding_strategy=padding_strategy,\n            symbol_based_hypothesis=symbol_based_hypothesis\n        )\n        features: List[ClassificationFeatures] = list(\n            tqdm(\n                p.imap(annotate_, examples, chunksize=32),\n                total=len(examples),\n                desc=\"convert examples to features\",\n                disable=not tqdm_enabled,\n            )\n        )\n    new_features = []\n    for example_index, example_features in enumerate(features):\n        example_features.example_index = example_index\n        example_features.unique_id = example_index + 1000000000\n        new_features.append(example_features)\n    features: List[ClassificationFeatures] = new_features\n    del new_features\n    assert len(features) == len(examples)\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n\n    all_feature_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n\n    all_class_label = torch.tensor(\n        [f.class_label for f in features], dtype=torch.long)\n\n    dataset = [\n        all_input_ids,\n        all_attention_masks,\n        all_token_type_ids,\n        all_cls_index,\n        all_p_mask,\n        all_feature_index,\n        all_class_label\n    ]\n    dataset = TensorDataset(*dataset)\n    return features, dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.689620Z","iopub.execute_input":"2024-11-18T22:13:45.689944Z","iopub.status.idle":"2024-11-18T22:13:45.714776Z","shell.execute_reply.started":"2024-11-18T22:13:45.689914Z","shell.execute_reply":"2024-11-18T22:13:45.713736Z"},"trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.\n# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/0c9bae09340dd8c6fdf6aa2ea5637e956efe0f7c/examples/question-answering/run.py\n# See git log for changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport os\nfrom typing import Tuple, List, Optional, Union\n\nimport torch\nfrom torch.utils.data import TensorDataset\n\n# from contract_nli.dataset.encoder import convert_examples_to_features, \\\n#     IdentificationClassificationFeatures\n# from contract_nli.dataset.encoder_classification import convert_examples_to_features as convert_examples_to_classification_features\n# from contract_nli.dataset.encoder_classification import ClassificationFeatures\n# from contract_nli.dataset.loader import ContractNLIExample\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_and_cache_examples(\n        path: str, *, local_rank: int = 1, overwrite_cache = False,\n        cache_dir: str = '.') -> List[ContractNLIExample]:\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        pass\n    filename = os.path.splitext(os.path.basename(path))[0]\n    cachename = f'cached_examples_{filename}'\n    cached_examples_file = os.path.join(cache_dir, cachename)\n\n    # Init features and dataset from cache if it exists\n    if os.path.exists(cached_examples_file) and not overwrite_cache:\n        logger.info(\"Loading examples from cached file %s\", cached_examples_file)\n        features_and_dataset = torch.load(cached_examples_file)\n        examples = features_and_dataset[\"examples\"]\n    else:\n        assert local_rank in [-1, 0]\n        logger.info(f\"Creating examples from dataset file at {path}\")\n        with open(path) as fin:\n            input_dict = json.load(fin)\n            \n            # slef made change\n#             print(\"old_len: \",len(input_dict['documents']))\n#             input_dict['documents'] = input_dict['documents'][: len(input_dict['documents']) // 5]\n#             print(\"new_len: \",len(input_dict['documents']))\n            \n        examples = ContractNLIExample.load(input_dict)\n\n\n        logger.info(\"Saving examples into cached file %s\", cached_examples_file)\n        torch.save({\"examples\": examples}, cached_examples_file)\n\n    return examples\n\n\ndef load_and_cache_features(\n        path: str, examples: List[ContractNLIExample], tokenizer, *,\n        max_seq_length: int, doc_stride: int, max_query_length: int,\n        dataset_type: str, symbol_based_hypothesis: bool,\n        threads: Optional[int] = 1, local_rank: int = 1,\n        overwrite_cache = False, labels_available=True, cache_dir: str = '.'\n        ) -> Tuple[TensorDataset, List[Union[IdentificationClassificationFeatures, ClassificationFeatures]]]:\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        pass\n    filename = os.path.splitext(os.path.basename(path))[0]\n    tokenizer_name = os.path.splitext(os.path.split(tokenizer.name_or_path)[-1])[0]\n    cachename = f'cached_features_{filename}_{dataset_type}_{tokenizer_name}_{max_seq_length}_{max_query_length}_{doc_stride}'\n    if not labels_available:\n        cachename += '_nolabels'\n    cached_features_file = os.path.join(cache_dir, cachename)\n\n    # Init features and dataset from cache if it exists\n    if os.path.exists(cached_features_file) and not overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        features, dataset = (\n            features_and_dataset[\"features\"],\n            features_and_dataset[\"dataset\"]\n        )\n    else:\n        assert local_rank in [-1, 0]\n        logger.info(f\"Creating features from dataset file at {path}\")\n        if dataset_type == 'identification_classification':\n            features, dataset = convert_examples_to_features(\n                examples=examples,\n                tokenizer=tokenizer,\n                max_seq_length=max_seq_length,\n                doc_stride=doc_stride,\n                max_query_length=max_query_length,\n                labels_available=labels_available,\n                symbol_based_hypothesis=symbol_based_hypothesis,\n                threads=threads\n            )\n        elif dataset_type == 'classification':\n            features, dataset = convert_examples_to_classification_features(\n                examples=examples,\n                tokenizer=tokenizer,\n                max_seq_length=max_seq_length,\n                max_query_length=max_query_length,\n                symbol_based_hypothesis=symbol_based_hypothesis,\n                threads=threads\n            )\n        else:\n            assert not \"dataset_type must be either 'classification' or 'identification_classification'\"\n\n        logger.info(\"Saving features into cached file %s\", cached_features_file)\n        torch.save({\"features\": features, \"dataset\": dataset}, cached_features_file)\n\n    return dataset, features\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.716613Z","iopub.execute_input":"2024-11-18T22:13:45.717127Z","iopub.status.idle":"2024-11-18T22:13:45.733159Z","shell.execute_reply.started":"2024-11-18T22:13:45.717090Z","shell.execute_reply":"2024-11-18T22:13:45.732236Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\nfrom typing import Dict, List, Union\n\nimport numpy as np\nimport sklearn.metrics\nfrom scipy.stats import hmean\n\n# from contract_nli.dataset.loader import NLILabel, ContractNLIExample\n# from contract_nli.postprocess import IdentificationClassificationResult, ClassificationResult\n\n\ndef evaluate_predicted_spans(y_true, y_pred) -> Dict[str, float]:\n    if y_true.sum() == 0:\n        # do not use zero_division=np.nan because it cannot distinguish\n        # zero divisions from y_true and y_pred in f1_score\n        recall = np.nan\n        f1 = np.nan\n    else:\n        recall = sklearn.metrics.recall_score(y_true, y_pred)\n        f1 = sklearn.metrics.f1_score(y_true, y_pred, zero_division=0)\n\n    return {\n        'precision': sklearn.metrics.precision_score(y_true, y_pred, zero_division=0),\n        'recall': recall,\n        'f1': f1,\n        'accuracy': sklearn.metrics.accuracy_score(y_true, y_pred),\n    }\n\n\ndef precision_at_recall(y_true, y_prob, recall: float):\n    assert 0. <= recall <= 1.0\n    if len(y_true) == 0 or np.sum(y_true) == 0:\n        return np.nan\n    threshs = np.sort(np.unique(y_prob))[::-1]\n    # (len(np.unique(y_prob)), len(y_prob)) where first axis show prediction at different thresh\n    y_preds = y_prob[None, :] >= threshs[:, None]\n    recalls = np.logical_and(y_true[None, :], y_preds).sum(axis=1) / np.sum(y_true)\n    # check that recalls are monotonically increasing\n    assert np.all(recalls == np.sort(recalls))\n    # because of >= relationship, there exist at least one thresh that gives\n    # recall score of 1.0\n    thresh = threshs[np.where(recalls >= recall)[0][0]]\n    y_pred = y_prob >= thresh\n    return sklearn.metrics.precision_score(y_true, y_pred, zero_division=0.)\n\n\ndef evaluate_spans(y_true, y_prob) -> Dict[str, float]:\n    assert y_prob.ndim == 1\n    assert y_true.ndim == 1\n    assert len(y_true) == len(y_prob)\n    metrics = evaluate_predicted_spans(y_true, y_prob > 0.5)\n    metrics.update({\n        'roc_auc': sklearn.metrics.roc_auc_score(y_true, y_prob),\n        'map': sklearn.metrics.average_precision_score(y_true, y_prob),\n        'precision@recall80': precision_at_recall(y_true, y_prob, 0.8),\n        'precision@recall90': precision_at_recall(y_true, y_prob, 0.9)\n    })\n    return metrics\n\n\ndef predict_at_k(y_prob, k):\n    y_pred = np.zeros_like(y_prob)\n    for j in np.argsort(y_prob)[::-1][:k]:\n        y_pred[j] = 1\n    assert y_pred.sum() == min(k, len(y_pred))\n    return y_pred\n\n\ndef evaluate_class(y_true, y_prob) -> Dict[str, float]:\n    assert y_prob.ndim == 2 and y_prob.shape[1] == 3\n    assert y_true.ndim == 1\n    assert len(y_true) == len(y_prob)\n    y_pred = np.argmax(y_prob, axis=1)\n    metrics = {\n        'accuracy': sklearn.metrics.accuracy_score(y_true, y_pred)\n    }\n    for label in (NLILabel.ENTAILMENT, NLILabel.CONTRADICTION):\n        ln = label.name.lower()\n        _y_true = y_true == label.value\n        _y_pred = y_pred == label.value\n        if _y_true.sum() == 0:\n            # do not use zero_division=np.nan because it cannot distinguish\n            # zero divisions from y_true and y_pred in f1_score\n            recall = np.nan\n            f1 = np.nan\n        else:\n            recall = sklearn.metrics.recall_score(_y_true, _y_pred)\n            f1 = sklearn.metrics.f1_score(_y_true, _y_pred, zero_division=0)\n        metrics.update({\n            f'precision_{ln}': sklearn.metrics.precision_score(_y_true, _y_pred, zero_division=0),\n            f'recall_{ln}': recall,\n            f'f1_{ln}': f1,\n        })\n    for m in ('precision', 'recall', 'f1'):\n        m_e = metrics[f'{m}_{NLILabel.ENTAILMENT.name.lower()}']\n        m_c = metrics[f'{m}_{NLILabel.CONTRADICTION.name.lower()}']\n        if np.isnan(m_e) or np.isnan(m_c):\n            metrics[f'{m}_mean'] = np.nan\n            metrics[f'{m}_hmean'] = np.nan\n        else:\n            metrics[f'{m}_mean'] = np.mean((m_e, m_c))\n            metrics[f'{m}_hmean'] = hmean((m_e, m_c))\n\n    return metrics\n\n\ndef _macro_average(dicts: List[Dict[str, float]]):\n    ret = dict()\n    for k in dicts[0].keys():\n        vals = [d[k] for d in dicts if not np.isnan(d[k])]\n        ret[k] = sum(vals) / float(len(vals))\n    return ret\n\n\ndef remove_not_mentioned(y_pred):\n    assert y_pred.shape[1] == 3\n    y_bin = y_pred[:, [NLILabel.CONTRADICTION.value, NLILabel.ENTAILMENT.value]]\n    y_bin = np.where(np.tile(np.sum(y_bin, axis=1, keepdims=True), [1, 2]) == 0,\n                     0.5,\n                     y_bin / np.sum(y_bin, axis=1, keepdims=True))\n    y_pred = np.zeros((len(y_pred), 3), dtype=y_pred.dtype)\n    y_pred[:, [NLILabel.CONTRADICTION.value, NLILabel.ENTAILMENT.value]] = y_bin\n    return y_pred\n\n\ndef evaluate_all(\n        dataset: dict,\n        results: List[dict],\n        ks: List[int],\n        task: str\n        ) -> dict:\n    assert task in ['identification_classification', 'classification', 'identification']\n    id_to_result = {r['id']: r for r in results}\n    label_ids = sorted(results[0]['annotation_sets'][0]['annotations'].keys())\n    class_names = [NLILabel(i).to_anno_name() for i in range(len(NLILabel))]\n    assert label_ids == sorted(dataset['labels'].keys()) or task == 'classification'\n    if task in ['identification_classification', 'identification']:\n        span_probs = defaultdict(list)\n        span_labels = defaultdict(list)\n    if task in ['identification_classification', 'classification']:\n        class_probs = defaultdict(list)\n        class_labels = defaultdict(list)\n    for document in dataset['documents']:\n        result = id_to_result[document['id']]['annotation_sets'][0]['annotations']\n        annotations = document['annotation_sets'][0]['annotations']\n        for label_id in label_ids:\n            if task == 'classification' and label_id not in result:\n                continue\n            if task in ['identification_classification', 'classification']:\n                class_labels[label_id].append(NLILabel.from_str(annotations[label_id]['choice']).value)\n                class_probs[label_id].append(\n                    np.array([result[label_id]['class_probs'][n] for n in class_names]))\n            if task in ['identification_classification', 'identification']:\n                # FIXME: this calculates precision optimistically\n                if NLILabel.from_str(annotations[label_id]['choice']) != NLILabel.NOT_MENTIONED:\n                    span_label = np.zeros(len(document['spans']))\n                    for s in annotations[label_id]['spans']:\n                        span_label[s] = 1\n                    span_labels[label_id].append(span_label)\n                    span_probs[label_id].append(np.array(result[label_id]['span_probs']))\n    \n    for x in label_ids:\n        if x not in span_probs:\n            label_ids.remove(x)\n    \n    \n    if task in ['identification_classification', 'classification']:\n        binary_label_ids = [\n            l for l in label_ids\n            if NLILabel.CONTRADICTION.value in class_labels[l] and\n               NLILabel.ENTAILMENT.value in class_labels[l]]\n        # this is not necessarily true with some training dataset\n        # but we have to assume this for our evaluation to be a fair comparison\n        if not set(class_probs.keys()).issuperset(set(binary_label_ids)):\n            raise ValueError(\n                'Some label ids are not in prediction when they are valid label '\n                f'ids. Pred: {class_probs.keys()}, Dataset: {binary_label_ids}')\n    if task in ['identification_classification', 'identification']:\n        preds_at_ks = {\n            k: {label_id: [predict_at_k(y_prob, k) for y_prob in y_probs]\n                for label_id, y_probs in span_probs.items()}\n            for k in ks\n        }\n\n    metrics = dict()\n\n    metrics['micro_label_micro_doc'] = dict()\n    if task in ['identification_classification', 'classification']:\n        metrics['micro_label_micro_doc']['class_binary'] = evaluate_class(\n            np.concatenate([np.array(class_labels[l])[np.array(class_labels[l]) != NLILabel.NOT_MENTIONED.value]\n                            for l in binary_label_ids]),\n            remove_not_mentioned(\n                np.vstack([np.stack(class_probs[l])[np.array(class_labels[l]) != NLILabel.NOT_MENTIONED.value, :]\n                           for l in binary_label_ids]))\n        )\n    if task == 'identification_classification':\n        metrics['micro_label_micro_doc']['class'] = evaluate_class(\n            np.concatenate([class_labels[l] for l in label_ids]),\n            np.vstack([np.stack(class_probs[l]) for l in label_ids])\n        )\n    if task in ['identification_classification', 'identification']:\n        y_true = np.concatenate([l for k in label_ids for l in span_labels[k]])\n        metrics['micro_label_micro_doc']['span'] = evaluate_spans(\n            y_true,\n            np.concatenate([l for l in label_ids for l in span_probs[l]])\n        )\n        for k in ks:\n            y_pred = np.concatenate([p for l in label_ids for p in preds_at_ks[k][l]])\n            metrics['micro_label_micro_doc']['span'].update({\n                f'{n}@{k}': v for n, v in evaluate_predicted_spans(y_true, y_pred).items()\n            })\n    metrics['macro_label_micro_doc'] = dict()\n    if task in ['identification_classification', 'classification']:\n        metrics['macro_label_micro_doc']['class_binary'] = _macro_average([\n            evaluate_class(\n                np.array(class_labels[l])[np.array(class_labels[l]) != NLILabel.NOT_MENTIONED.value],\n                remove_not_mentioned(np.stack(class_probs[l])[np.array(class_labels[l]) != NLILabel.NOT_MENTIONED.value, :]))\n            for l in binary_label_ids\n        ])\n    if task == 'identification_classification':\n        metrics['macro_label_micro_doc']['class'] = _macro_average([\n            evaluate_class(np.array(class_labels[l]), np.stack(class_probs[l]))\n            for l in label_ids\n        ])\n    if task in ['identification_classification', 'identification']:\n        metrics['macro_label_micro_doc']['span'] = _macro_average([\n            {\n                **evaluate_spans(\n                    np.concatenate(span_labels[l]),\n                    np.concatenate(span_probs[l])),\n                **{\n                    f'{n}@{k}': v\n                    for k in ks\n                    for n, v in evaluate_predicted_spans(\n                        np.concatenate(span_labels[l]),\n                        np.concatenate(preds_at_ks[k][l])).items()\n                }\n            }\n            for l in label_ids\n        ])\n\n    if task in ['identification_classification', 'identification']:\n        metrics['macro_label_macro_doc'] = dict()\n        metrics['macro_label_macro_doc']['span'] = _macro_average([\n            _macro_average([\n                {\n                   **evaluate_spans(span_labels[l][i], span_probs[l][i]),\n                   **{\n                       f'{n}@{k}': v\n                       for k in ks\n                       for n, v in evaluate_predicted_spans(\n                           span_labels[l][i],\n                           preds_at_ks[k][l][i]).items()\n                   }\n                }\n                for i in range(len(span_labels[l]))\n            ])\n            for l in label_ids\n        ])\n\n    metrics['micro_label_macro_doc'] = dict()\n    if task == 'identification_classification':\n        metrics['micro_label_macro_doc']['class'] = _macro_average([\n            evaluate_class(\n                np.array([class_labels[l][i] for l in label_ids]),\n                np.stack([class_probs[l][i] for l in label_ids]))\n            for i in range(len(class_labels[label_ids[0]]))\n        ])\n    if task in ['identification_classification', 'identification']:\n        metrics['micro_label_macro_doc']['span'] = _macro_average([\n            evaluate_spans(_l, _p)\n            for l in label_ids\n            for _l, _p in zip(span_labels[l], span_probs[l])\n        ])\n        metrics['micro_label_macro_doc']['span'].update({\n            key: value\n            for k in ks\n            for key, value in _macro_average([\n                {\n                    f'{n}@{k}': v\n                    for n, v in evaluate_predicted_spans(_l, _p).items()\n                }\n                for l in label_ids\n                for _l, _p in zip(span_labels[l], preds_at_ks[k][l])\n            ]).items()\n        })\n\n    metrics['label_wise'] = dict()\n    for l in label_ids:\n        metrics['label_wise'][l] = dict()\n        metrics['label_wise'][l]['micro_doc'] = dict()\n        if task in ['identification_classification', 'classification']:\n            metrics['label_wise'][l]['micro_doc']['class_binary'] = evaluate_class(\n                np.array(class_labels[l])[np.array(class_labels[l]) != NLILabel.NOT_MENTIONED.value],\n                remove_not_mentioned(np.stack(class_probs[l])[np.array(class_labels[l]) != NLILabel.NOT_MENTIONED.value, :]))\n            if not (NLILabel.CONTRADICTION.value in class_labels[l] and NLILabel.ENTAILMENT.value in class_labels[l]):\n                metrics['label_wise'][l]['micro_doc']['class_binary'] = {\n                    k: np.nan for k in metrics['label_wise'][l]['micro_doc']['class_binary'].keys()\n                }\n        if task == 'identification_classification':\n            metrics['label_wise'][l]['micro_doc']['class'] = evaluate_class(\n                np.array(class_labels[l]), np.stack(class_probs[l]))\n        if task in ['identification_classification', 'identification']:\n            y_true = np.concatenate(span_labels[l])\n            metrics['label_wise'][l]['micro_doc']['span'] = {\n                **evaluate_spans(y_true, np.concatenate(span_probs[l])),\n                **{\n                    f'{n}@{k}': v\n                    for k in ks\n                    for n, v in evaluate_predicted_spans(\n                        y_true, np.concatenate(preds_at_ks[k][l])).items()\n                }\n            }\n            metrics['label_wise'][l]['macro_doc'] = dict()\n            metrics['label_wise'][l]['macro_doc']['span'] = _macro_average([\n                {\n                    **evaluate_spans(span_labels[l][i], span_probs[l][i]),\n                    **{\n                        f'{n}@{k}': v\n                        for k in ks\n                        for n, v in evaluate_predicted_spans(\n                            span_labels[l][i], preds_at_ks[k][l][i]).items()\n                    }\n                }\n                for i in range(len(span_labels[l]))\n            ])\n    return metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.787111Z","iopub.execute_input":"2024-11-18T22:13:45.787461Z","iopub.status.idle":"2024-11-18T22:13:45.838040Z","shell.execute_reply.started":"2024-11-18T22:13:45.787429Z","shell.execute_reply":"2024-11-18T22:13:45.837017Z"},"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nfrom transformers.file_utils import ModelOutput\n\n\n@dataclass\nclass IdentificationClassificationModelOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    loss_cls: Optional[torch.FloatTensor] = None\n    loss_span: Optional[torch.FloatTensor] = None\n    class_logits: torch.FloatTensor = None\n    span_logits: torch.FloatTensor = None","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.839981Z","iopub.execute_input":"2024-11-18T22:13:45.840531Z","iopub.status.idle":"2024-11-18T22:13:45.851623Z","shell.execute_reply.started":"2024-11-18T22:13:45.840500Z","shell.execute_reply":"2024-11-18T22:13:45.850903Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch import nn\nfrom transformers.models.bert import BertPreTrainedModel, BertModel\nfrom transformers.utils import logging\n\n# from contract_nli.dataset.loader import NLILabel\n# from contract_nli.model.identification_classification.model_output import \\\n#     IdentificationClassificationModelOutput\n\nlogger = logging.get_logger(__name__)\n\n\nclass BertForIdentificationClassification(BertPreTrainedModel):\n\n    IMPOSSIBLE_STRATEGIES = {'ignore', 'label', 'not_mentioned'}\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config, add_pooling_layer=True)\n        self.class_outputs = nn.Linear(config.hidden_size, 3)\n        self.span_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        self.model_type: str = config.model_type\n\n        if config.impossible_strategy not in self.IMPOSSIBLE_STRATEGIES:\n            raise ValueError(\n                f'impossible_strategy must be one of {self.IMPOSSIBLE_STRATEGIES}')\n        self.impossible_strategy = config.impossible_strategy\n\n        self.class_loss_weight = config.class_loss_weight\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        class_labels=None,\n        span_labels=None,\n        p_mask=None,\n        valid_span_missing_in_context=None,\n    ) -> IdentificationClassificationModelOutput:\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=False,\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        sequence_output = outputs.last_hidden_state\n        pooled_output = outputs.pooler_output\n\n        pooled_output = self.dropout(pooled_output)\n        logits_cls = self.class_outputs(pooled_output)\n\n        sequence_output = self.dropout(sequence_output)\n        logits_span = self.span_outputs(sequence_output)\n\n        if class_labels is not None:\n            assert p_mask is not None\n            assert span_labels is not None\n            assert valid_span_missing_in_context is not None\n\n            loss_fct = nn.CrossEntropyLoss()\n            if self.impossible_strategy == 'ignore':\n                class_labels = torch.where(\n                    valid_span_missing_in_context == 0, class_labels,\n                    torch.tensor(loss_fct.ignore_index).type_as(class_labels)\n                )\n            elif self.impossible_strategy == 'not_mentioned':\n                class_labels = torch.where(\n                    valid_span_missing_in_context == 0, class_labels,\n                    NLILabel.NOT_MENTIONED.value\n                )\n            loss_cls = self.class_loss_weight * loss_fct(logits_cls, class_labels)\n\n            loss_fct = nn.CrossEntropyLoss()\n            active_logits = logits_span.view(-1, 2)\n            active_labels = torch.where(\n                p_mask.view(-1) == 0, span_labels.view(-1),\n                torch.tensor(loss_fct.ignore_index).type_as(span_labels)\n            )\n            loss_span = loss_fct(active_logits, active_labels)\n            loss = loss_cls + loss_span\n        else:\n            loss, loss_cls, loss_span = None, None, None\n\n        return IdentificationClassificationModelOutput(\n            loss=loss,\n            loss_cls=loss_cls,\n            loss_span=loss_span,\n            class_logits=logits_cls,\n            span_logits=logits_span\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.853004Z","iopub.execute_input":"2024-11-18T22:13:45.853359Z","iopub.status.idle":"2024-11-18T22:13:45.867611Z","shell.execute_reply.started":"2024-11-18T22:13:45.853324Z","shell.execute_reply":"2024-11-18T22:13:45.866729Z"},"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef update_config(config, *, impossible_strategy, class_loss_weight):\n    class IdentificationClassificationConfig(type(config)):\n        def __init__(self, impossible_strategy='ignore',\n                     class_loss_weight=1.0, **kwargs):\n            super().__init__(**kwargs)\n            self.impossible_strategy = impossible_strategy\n            self.class_loss_weight = class_loss_weight\n\n        @classmethod\n        def from_config(cls, config, *, impossible_strategy,\n                        class_loss_weight):\n            kwargs = config.to_dict()\n            assert 'impossible_strategy' not in kwargs\n            kwargs['impossible_strategy'] = impossible_strategy\n            assert 'class_loss_weight' not in kwargs\n            kwargs['class_loss_weight'] = class_loss_weight\n            return cls(**kwargs)\n\n    return IdentificationClassificationConfig.from_config(\n        config, impossible_strategy=impossible_strategy,\n        class_loss_weight=class_loss_weight)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.868686Z","iopub.execute_input":"2024-11-18T22:13:45.868969Z","iopub.status.idle":"2024-11-18T22:13:45.881972Z","shell.execute_reply.started":"2024-11-18T22:13:45.868944Z","shell.execute_reply":"2024-11-18T22:13:45.881303Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch import nn\nfrom transformers.models.deberta import DebertaPreTrainedModel, DebertaModel\nfrom transformers.utils import logging\n\n# from contract_nli.dataset.loader import NLILabel\n# from contract_nli.model.identification_classification.model_output import \\\n#     IdentificationClassificationModelOutput\n\nlogger = logging.get_logger(__name__)\n\n\nclass DeBertaForIdentificationClassification(DebertaPreTrainedModel):\n\n    IMPOSSIBLE_STRATEGIES = {'ignore', 'label', 'not_mentioned'}\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.deberta = DebertaModel(config)\n        self.class_outputs = nn.Linear(config.hidden_size, 3)\n        self.span_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        self.model_type: str = config.model_type\n\n        if config.impossible_strategy not in self.IMPOSSIBLE_STRATEGIES:\n            raise ValueError(\n                f'impossible_strategy must be one of {self.IMPOSSIBLE_STRATEGIES}')\n        self.impossible_strategy = config.impossible_strategy\n\n        self.class_loss_weight = config.class_loss_weight\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        class_labels=None,\n        span_labels=None,\n        p_mask=None,\n        valid_span_missing_in_context=None,\n    ) -> IdentificationClassificationModelOutput:\n        outputs = self.deberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=False,\n            output_hidden_states=True,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        # FIXME: hardcoded [CLS] token index of 0 is might not be appriopriate\n        pooled_output = sequence_output[:, 0]\n\n        pooled_output = self.dropout(pooled_output)\n        logits_cls = self.class_outputs(pooled_output)\n\n        sequence_output = self.dropout(sequence_output)\n        logits_span = self.span_outputs(sequence_output)\n\n        if class_labels is not None:\n            assert p_mask is not None\n            assert span_labels is not None\n            assert valid_span_missing_in_context is not None\n\n            loss_fct = nn.CrossEntropyLoss()\n            if self.impossible_strategy == 'ignore':\n                class_labels = torch.where(\n                    valid_span_missing_in_context == 0, class_labels,\n                    torch.tensor(loss_fct.ignore_index).type_as(class_labels)\n                )\n            elif self.impossible_strategy == 'not_mentioned':\n                class_labels = torch.where(\n                    valid_span_missing_in_context == 0, class_labels,\n                    NLILabel.NOT_MENTIONED.value\n                )\n            loss_cls = self.class_loss_weight * loss_fct(logits_cls, class_labels)\n\n            loss_fct = nn.CrossEntropyLoss()\n            active_logits = logits_span.view(-1, 2)\n            active_labels = torch.where(\n                p_mask.view(-1) == 0, span_labels.view(-1),\n                torch.tensor(loss_fct.ignore_index).type_as(span_labels)\n            )\n            loss_span = loss_fct(active_logits, active_labels)\n            loss = loss_cls + loss_span\n        else:\n            loss, loss_cls, loss_span = None, None, None\n\n        return IdentificationClassificationModelOutput(\n            loss=loss,\n            loss_cls=loss_cls,\n            loss_span=loss_span,\n            class_logits=logits_cls,\n            span_logits=logits_span\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.884188Z","iopub.execute_input":"2024-11-18T22:13:45.884514Z","iopub.status.idle":"2024-11-18T22:13:45.898160Z","shell.execute_reply.started":"2024-11-18T22:13:45.884480Z","shell.execute_reply":"2024-11-18T22:13:45.897265Z"},"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers.models.deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model\nfrom transformers.utils import logging\n\n# from contract_nli.dataset.loader import NLILabel\n# from contract_nli.model.identification_classification.model_output import \\\n#     IdentificationClassificationModelOutput\n\nlogger = logging.get_logger(__name__)\n\n\nclass DeBertaV2ForIdentificationClassification(DebertaV2PreTrainedModel):\n\n    IMPOSSIBLE_STRATEGIES = {'ignore', 'label', 'not_mentioned'}\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.deberta = DebertaV2Model(config)\n        self.class_outputs = nn.Linear(config.hidden_size, 3)\n        self.span_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        self.model_type: str = config.model_type\n\n        if config.impossible_strategy not in self.IMPOSSIBLE_STRATEGIES:\n            raise ValueError(\n                f'impossible_strategy must be one of {self.IMPOSSIBLE_STRATEGIES}')\n        self.impossible_strategy = config.impossible_strategy\n\n        self.class_loss_weight = config.class_loss_weight\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        class_labels=None,\n        span_labels=None,\n        p_mask=None,\n        valid_span_missing_in_context=None,\n    ) -> IdentificationClassificationModelOutput:\n        outputs = self.deberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=False,\n            output_hidden_states=True,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        # FIXME: hardcoded [CLS] token index of 0 is might not be appriopriate\n        pooled_output = sequence_output[:, 0]\n\n        pooled_output = self.dropout(pooled_output)\n        logits_cls = self.class_outputs(pooled_output)\n\n        sequence_output = self.dropout(sequence_output)\n        logits_span = self.span_outputs(sequence_output)\n\n        if class_labels is not None:\n            assert p_mask is not None\n            assert span_labels is not None\n            assert valid_span_missing_in_context is not None\n\n            loss_fct = nn.CrossEntropyLoss()\n            if self.impossible_strategy == 'ignore':\n                class_labels = torch.where(\n                    valid_span_missing_in_context == 0, class_labels,\n                    torch.tensor(loss_fct.ignore_index).type_as(class_labels)\n                )\n            elif self.impossible_strategy == 'not_mentioned':\n                class_labels = torch.where(\n                    valid_span_missing_in_context == 0, class_labels,\n                    NLILabel.NOT_MENTIONED.value\n                )\n            loss_cls = self.class_loss_weight * loss_fct(logits_cls, class_labels)\n\n            loss_fct = nn.CrossEntropyLoss()\n            active_logits = logits_span.view(-1, 2)\n            active_labels = torch.where(\n                p_mask.view(-1) == 0, span_labels.view(-1),\n                torch.tensor(loss_fct.ignore_index).type_as(span_labels)\n            )\n            loss_span = loss_fct(active_logits, active_labels)\n            loss = loss_cls + loss_span\n        else:\n            loss, loss_cls, loss_span = None, None, None\n\n        return IdentificationClassificationModelOutput(\n            loss=loss,\n            loss_cls=loss_cls,\n            loss_span=loss_span,\n            class_logits=logits_cls,\n            span_logits=logits_span\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.899439Z","iopub.execute_input":"2024-11-18T22:13:45.899738Z","iopub.status.idle":"2024-11-18T22:13:45.914031Z","shell.execute_reply.started":"2024-11-18T22:13:45.899714Z","shell.execute_reply":"2024-11-18T22:13:45.913212Z"},"trusted":true},"outputs":[],"execution_count":51},{"cell_type":"code","source":"MODEL_TYPE_TO_CLASS = {\n    'bert': BertForIdentificationClassification,\n    'deberta': DeBertaForIdentificationClassification,\n    'deberta-v2': DeBertaV2ForIdentificationClassification\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.915152Z","iopub.execute_input":"2024-11-18T22:13:45.915397Z","iopub.status.idle":"2024-11-18T22:13:45.926513Z","shell.execute_reply.started":"2024-11-18T22:13:45.915375Z","shell.execute_reply":"2024-11-18T22:13:45.925778Z"},"trusted":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\nfrom transformers.file_utils import ModelOutput\nfrom transformers.models.bert import BertPreTrainedModel, BertModel\nfrom transformers.utils import logging\nimport numpy as np\n\n# from contract_nli.dataset.loader import NLILabel\n\nlogger = logging.get_logger(__name__)\n\n\n@dataclass\nclass ClassificationModelOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    loss_cls: Optional[torch.FloatTensor] = None\n    class_logits: torch.FloatTensor = None\n\n\nclass BertForClassification(BertPreTrainedModel):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config, add_pooling_layer=True)\n        self.class_outputs = nn.Linear(config.hidden_size, 3)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        self.model_type: str = config.model_type\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        class_labels=None,\n        p_mask=None,\n    ) -> ClassificationModelOutput:\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=False,\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        logits_cls = self.class_outputs(pooled_output)\n\n        mask = np.ones((len(logits_cls), 3))\n        mask[:, NLILabel.NOT_MENTIONED.value] = 0\n        mask = torch.tensor(mask, requires_grad=False, device=logits_cls.device)\n        logits_cls = torch.where(\n            mask.bool(), logits_cls,\n            torch.tensor(-1000000).type_as(logits_cls)\n        )\n\n        if class_labels is not None:\n            assert p_mask is not None\n\n            loss_fct = nn.CrossEntropyLoss()\n            loss_cls = loss_fct(logits_cls, class_labels)\n            loss = loss_cls\n        else:\n            loss, loss_cls = None, None\n\n        return ClassificationModelOutput(\n            loss=loss,\n            loss_cls=loss_cls,\n            class_logits=logits_cls\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.927714Z","iopub.execute_input":"2024-11-18T22:13:45.927972Z","iopub.status.idle":"2024-11-18T22:13:45.941327Z","shell.execute_reply.started":"2024-11-18T22:13:45.927950Z","shell.execute_reply":"2024-11-18T22:13:45.940602Z"},"trusted":true},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nfrom typing import List, Union, Optional\n\nimport numpy as np\nfrom scipy.special import softmax\n\n# from contract_nli.dataset.encoder import IdentificationClassificationFeatures\n# from contract_nli.dataset.loader import ContractNLIExample, NLILabel\n\n\nclass IdentificationClassificationPartialResult:\n    def __init__(self, unique_id, class_logits, span_logits):\n        self.class_logits = class_logits\n        self.span_logits = span_logits\n        self.unique_id = unique_id\n\n\nclass IdentificationClassificationResult:\n    def __init__(self, data_id, class_probs, span_probs):\n        self.class_probs = class_probs\n        self.span_probs = span_probs\n        self.data_id = data_id\n\n\nclass ClassificationResult:\n    def __init__(self, data_id, class_probs):\n        self.class_probs = class_probs\n        self.data_id = data_id\n\n\ndef compute_predictions_logits(\n        all_examples: List[ContractNLIExample],\n        all_features: List[IdentificationClassificationFeatures],\n        all_results: List[IdentificationClassificationPartialResult],\n        weight_class_probs_by_span_probs: bool,\n        calibration_coeff: Optional[float]\n        ) -> List[IdentificationClassificationResult]:\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    results = []\n    for example_index, example in enumerate(all_examples):\n        features: List[IdentificationClassificationFeatures] = example_index_to_features[example_index]\n        assert len(features) > 0\n        span_probs = np.zeros((len(example.splits), 2))\n        num_pred_spans = np.zeros(len(example.splits))\n        ave_span_probs = []\n        class_probs = []\n        for feature in features:\n            result = unique_id_to_result[feature.unique_id]\n            _span_probs = softmax(np.array(result.span_logits), axis=1)\n            for tok_idx, orig_span_indices in feature.span_to_orig_map.items():\n                for orig_span_idx in orig_span_indices:\n                    span_probs[orig_span_idx] += _span_probs[tok_idx]\n                    num_pred_spans[orig_span_idx] += 1\n            ave_span_probs.append(np.mean(_span_probs[:, 1]))\n            class_probs.append(softmax(result.class_logits))\n        assert np.all(num_pred_spans > 0)\n        span_probs /= num_pred_spans[:, None]\n        assert np.allclose(span_probs.sum(1), 1.0)\n        if weight_class_probs_by_span_probs:\n            ave_span_probs = np.array(ave_span_probs)\n            weight = ave_span_probs / ave_span_probs.sum()\n            class_probs = (np.array(class_probs) * weight[:, None]).sum(0)\n        else:\n            class_probs = np.array(class_probs).mean(0)\n        if calibration_coeff is not None:\n            class_ll = np.log(class_probs)\n            class_ll[0] -= calibration_coeff\n            class_probs = softmax(class_ll)\n        assert abs(1.0 - class_probs.sum()) < 0.001\n        results.append(IdentificationClassificationResult(\n            data_id=example.data_id,\n            span_probs=span_probs,\n            class_probs=class_probs\n        ))\n    return results\n\n\ndef format_json(\n        all_examples: List[ContractNLIExample],\n        all_results: List[Union[IdentificationClassificationResult, ClassificationResult]]\n        ) -> List[dict]:\n\n    data_id_to_result = {}\n    for result in all_results:\n        data_id_to_result[result.data_id] = result\n\n    documents = dict()\n    for example_index, example in enumerate(all_examples):\n        if example.document_id not in documents:\n            documents[example.document_id] = {\n                'id': example.document_id,\n                'file_name': example.file_name,\n                'text': example.context_text,\n                'spans': example.spans,\n                'annotation_sets': [{\n                    'user': 'prediction',\n                    'mturk': False,\n                    'annotations': dict()\n                }]\n            }\n        assert len(example.spans) == len(example.splits)\n        if example.data_id not in data_id_to_result:\n            assert isinstance(all_results[0], ClassificationResult)\n            continue\n        prediction = data_id_to_result[example.data_id]\n        d = {\n            'choice': NLILabel(\n                np.argmax(prediction.class_probs)).to_anno_name(),\n            'class_probs': {\n                NLILabel(i).to_anno_name(): float(p)\n                for i, p in enumerate(prediction.class_probs)\n            },\n        }\n        if isinstance(prediction, IdentificationClassificationResult):\n            d.update({\n                'spans': np.where(prediction.span_probs[:, 1] > 0.5)[0].tolist(),\n                'span_probs': prediction.span_probs[:, 1].tolist()\n            })\n        documents[example.document_id]['annotation_sets'][0]['annotations'][example.hypothesis_id] = d\n\n    if isinstance(all_results[0], IdentificationClassificationResult):\n        all_hypothesis_ids = [\n            tuple(sorted(document['annotation_sets'][0]['annotations'].keys()))\n            for document in documents.values()]\n        assert len(set(all_hypothesis_ids)) == 1\n    return sorted(documents.values(), key=lambda d: d['id'])\n\n\ndef compute_prob_calibration_coeff(\n        examples: List[ContractNLIExample],\n        results: List[IdentificationClassificationResult]):\n    data_id_to_result = {r.data_id: r for r in results}\n    y_prob = []\n    y_true = []\n    for example in examples:\n        y_true.append(example.label.value)\n        result = data_id_to_result[example.data_id]\n        y_prob.append(result.class_probs)\n    y_prob = np.array(y_prob)\n    y_true = np.array(y_true)\n    y_ll = np.log(y_prob)\n    # get all coeffs that can flip the prediction\n    coeffs = np.concatenate([\n        y_ll[:, 0] - y_ll[:, 1], y_ll[:, 0] - y_ll[:, 2], [0.0]])\n    # (len(threths), len(y_true), 3)\n    y_ll_all = y_ll[None, :, :] - np.concatenate((coeffs[:, None, None], np.zeros((len(coeffs), 1, 2))), axis=2)\n    y_pred_all = np.argmax(y_ll_all, axis=2)\n    accuracies = (y_true[None, :] == y_pred_all).mean(axis=1)\n    return coeffs[np.argmax(accuracies)]\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.943674Z","iopub.execute_input":"2024-11-18T22:13:45.943964Z","iopub.status.idle":"2024-11-18T22:13:45.969403Z","shell.execute_reply.started":"2024-11-18T22:13:45.943929Z","shell.execute_reply":"2024-11-18T22:13:45.968594Z"},"trusted":true},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\n\ndef identification_classification_converter(batch, model, device, no_labels=False) -> dict:\n    batch = tuple(t.to(device) for t in batch)\n    inputs = {\n        \"input_ids\": batch[0],\n        \"attention_mask\": batch[1],\n        \"token_type_ids\": batch[2],\n        \"p_mask\": batch[4],\n        \"valid_span_missing_in_context\": batch[5]\n    }\n    if not no_labels:\n        inputs[\"class_labels\"] = batch[7]\n        inputs[\"span_labels\"] = batch[8]\n\n    model_type = model.module.model_type if hasattr(model, \"module\") else model.model_type\n    if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n        del inputs[\"token_type_ids\"]\n\n    if model_type in [\"xlnet\", \"xlm\"]:\n        inputs.update({\"cls_index\": batch[3]})\n    # FIXME: Add lang_id to dataset\n    if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n        langs = torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id\n        inputs.update({\"langs\": langs.to(device)})\n    return inputs\n\n\ndef classification_converter(batch, model, device, no_labels=False) -> dict:\n    batch = tuple(t.to(device) for t in batch)\n    inputs = {\n        \"input_ids\": batch[0],\n        \"attention_mask\": batch[1],\n        \"token_type_ids\": batch[2],\n        \"p_mask\": batch[4],\n    }\n    if not no_labels:\n        inputs[\"class_labels\"] = batch[6]\n\n    model_type = model.module.model_type if hasattr(model, \"module\") else model.model_type\n    if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n        del inputs[\"token_type_ids\"]\n\n    if model_type in [\"xlnet\", \"xlm\"]:\n        inputs.update({\"cls_index\": batch[3]})\n    # FIXME: Add lang_id to dataset\n    if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n        langs = torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id\n        inputs.update({\"langs\": langs.to(device)})\n    return inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.971044Z","iopub.execute_input":"2024-11-18T22:13:45.971266Z","iopub.status.idle":"2024-11-18T22:13:45.985116Z","shell.execute_reply.started":"2024-11-18T22:13:45.971246Z","shell.execute_reply":"2024-11-18T22:13:45.984366Z"},"trusted":true},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.\n# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/0c9bae09340dd8c6fdf6aa2ea5637e956efe0f7c/examples/question-answering/run.py\n# See git log for changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nfrom typing import List, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.special import softmax\n\n# from contract_nli.model.identification_classification import \\\n#     IdentificationClassificationModelOutput\n# from contract_nli.postprocess import IdentificationClassificationPartialResult, \\\n#     compute_predictions_logits, IdentificationClassificationResult, ClassificationResult\n# from contract_nli.batch_converter import classification_converter, identification_classification_converter\n# from contract_nli.dataset.loader import NLILabel\n\nlogger = logging.getLogger(__name__)\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\n\ndef predict(model, dataset, examples, features, *, per_gpu_batch_size: int,\n            device, n_gpu: int, weight_class_probs_by_span_probs: bool,\n            calibration_coeff: Optional[float] = None\n            ) -> List[IdentificationClassificationResult]:\n    # We do not implement this as a part of Trainer, because we want to run\n    # inference without instanizing optimizers\n    eval_batch_size = per_gpu_batch_size * max(1, n_gpu)\n\n    # Do not use DistributedSampler because it samples randomly\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(\n        dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n\n    # multi-gpu evaluate\n    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n        model = torch.nn.DataParallel(model)\n\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(dataset))\n    logger.info(\"  Batch size = %d\", eval_batch_size)\n\n    all_results = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        inputs = identification_classification_converter(batch, model, device, no_labels=True)\n        with torch.no_grad():\n\n            feature_indices = batch[6]\n            outputs: IdentificationClassificationModelOutput = model(**inputs)\n\n        for i, feature_index in enumerate(feature_indices):\n            eval_feature = features[feature_index.item()]\n            unique_id = int(eval_feature.unique_id)\n\n            class_logits = to_list(outputs.class_logits[i])\n            span_logits = to_list(outputs.span_logits[i])\n            result = IdentificationClassificationPartialResult(\n                unique_id, class_logits, span_logits)\n\n            all_results.append(result)\n\n\n    all_results = compute_predictions_logits(\n        examples,\n        features,\n        all_results,\n        weight_class_probs_by_span_probs=weight_class_probs_by_span_probs,\n        calibration_coeff=calibration_coeff\n    )\n\n    return all_results\n\n\ndef predict_classification(model, dataset, features, *, per_gpu_batch_size: int,\n                           device, n_gpu: int) -> List[ClassificationResult]:\n    # We do not implement this as a part of Trainer, because we want to run\n    # inference without instanizing optimizers\n    eval_batch_size = per_gpu_batch_size * max(1, n_gpu)\n\n    # Do not use DistributedSampler because it samples randomly\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(\n        dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n\n    # multi-gpu evaluate\n    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n        model = torch.nn.DataParallel(model)\n\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(dataset))\n    logger.info(\"  Batch size = %d\", eval_batch_size)\n\n    label_inds = [NLILabel.ENTAILMENT.value, NLILabel.CONTRADICTION.value]\n\n    all_results = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        inputs = classification_converter(batch, model, device, no_labels=True)\n        with torch.no_grad():\n            feature_indices = batch[5]\n            outputs = model(**inputs)\n\n        for i, feature_index in enumerate(feature_indices):\n            eval_feature = features[feature_index.item()]\n            class_logits = outputs.class_logits[i].detach().cpu()\n            class_probs = np.zeros_like(class_logits)\n            class_probs[label_inds] = softmax(class_logits[label_inds])\n            result = ClassificationResult(eval_feature.data_id, class_probs.tolist())\n            all_results.append(result)\n\n    return all_results","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:45.986179Z","iopub.execute_input":"2024-11-18T22:13:45.986446Z","iopub.status.idle":"2024-11-18T22:13:46.003008Z","shell.execute_reply.started":"2024-11-18T22:13:45.986423Z","shell.execute_reply":"2024-11-18T22:13:46.002185Z"},"trusted":true},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\nfrom torch.utils.tensorboard import SummaryWriter as _SummaryWriter\n\n\nclass SummaryWriter(object):\n    def __init__(self, path: str, reduce_func=None):\n        if reduce_func is None:\n            reduce_func = lambda values: (sum(values) / len(values) if len(values) > 0 else None)\n        self.tb_writer = _SummaryWriter(path)\n        self.reduce_func = reduce_func\n        self.clear()\n\n    def __del__(self):\n            self.tb_writer.close()\n\n    def clear(self):\n        self.state = defaultdict(list)\n\n    def add_scalar(self, key, value, num: int=1):\n        self.state[key].extend([value] * num)\n\n    def write(self, global_step):\n        for key, values in self.state.items():\n            val = self.reduce_func(values)\n            if val is not None:\n                self.tb_writer.add_scalar(key, val, global_step)\n        self.clear()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:46.003929Z","iopub.execute_input":"2024-11-18T22:13:46.004246Z","iopub.status.idle":"2024-11-18T22:13:46.016945Z","shell.execute_reply.started":"2024-11-18T22:13:46.004213Z","shell.execute_reply":"2024-11-18T22:13:46.016198Z"},"trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.\n# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/0c9bae09340dd8c6fdf6aa2ea5637e956efe0f7c/examples/question-answering/run_squad.py\n# See git log for changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport glob\nimport json\nimport logging\nimport os\nfrom typing import Optional\nimport matplotlib.pyplot as plt\nimport math\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport sklearn.metrics\nimport scipy.special\n\n# from contract_nli.batch_converter import classification_converter, identification_classification_converter\n# from contract_nli.summary_writer import SummaryWriter\n\nlogger = logging.getLogger(__name__)\n\n\ndef setup_optimizer(model, learning_rate: float, epsilon: float, weight_decay: float):\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0\n        },\n    ]\n    return AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=epsilon)\n\n\nclass Trainer(object):\n    def __init__(\n            self, *, model, train_dataset, optimizer, task: str, output_dir: str,\n            per_gpu_train_batch_size: int, num_epochs: Optional[int] = None, max_steps: Optional[int] = None,\n            dev_dataset=None, valid_steps: Optional[int]=None, per_gpu_dev_batch_size: Optional[int]=None,\n            gradient_accumulation_steps: int=1, warmup_steps: int=0, max_grad_norm: Optional[float]=None,\n            n_gpu: int=1, local_rank: int=-1, fp16: bool=False, fp16_opt_level=None, device=torch.device(\"cpu\"),\n            save_steps: Optional[int] = None):\n        if local_rank in [-1, 0]:\n            self.tb_writer = SummaryWriter(os.path.join(output_dir, 'tensorboard'))\n        if task not in ['identification_classification', 'classification']:\n            raise ValueError(\"task must be either 'classification' or 'identification_classification'\")\n\n        train_batch_size = per_gpu_train_batch_size * max(1, n_gpu)\n        train_sampler = RandomSampler(train_dataset) if local_rank == -1 else DistributedSampler(train_dataset)\n        self.train_dataloader = DataLoader(\n            train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n\n        if dev_dataset is not None:\n            if per_gpu_dev_batch_size is None:\n                per_gpu_dev_batch_size = per_gpu_train_batch_size\n            dev_batch_size = per_gpu_dev_batch_size * max(1, n_gpu)\n            dev_sampler = RandomSampler(dev_dataset) if local_rank == -1 else DistributedSampler(dev_dataset)\n            self.dev_dataloader = DataLoader(\n                dev_dataset, sampler=dev_sampler, batch_size=dev_batch_size)\n        else:\n            self.dev_dataloader = None\n\n        if (num_epochs is None) == (max_steps is None):\n            raise ValueError('One and only one of num_epochs and max_steps can be specified')\n        if num_epochs is not None:\n            max_steps = len(self.train_dataloader) // gradient_accumulation_steps * num_epochs\n        else:\n            num_epochs = (max_steps * gradient_accumulation_steps) // len(self.train_dataloader)\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps\n        )\n\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.max_steps = max_steps\n        self.fp16 = fp16\n        self.fp16_opt_level = fp16_opt_level\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.device = device\n        self.n_gpu = n_gpu\n        self.local_rank = local_rank\n        self.valid_steps = valid_steps\n        self.max_grad_norm = max_grad_norm\n        self.per_gpu_train_batch_size = per_gpu_train_batch_size\n        self.output_dir = output_dir\n        self.save_steps = save_steps\n        self.task = task\n        if self.task == 'identification_classification':\n            self.converter = identification_classification_converter\n        else:\n            self.converter = classification_converter\n\n        self.global_step = 0\n        self.best_loss = np.inf\n\n        self.deployed = False\n\n        logger.info(\"***** Trainer *****\")\n        logger.info(\"  Num examples = %d\", len(train_dataset))\n        logger.info(\"  Instantaneous batch size per GPU = %d\", per_gpu_train_batch_size)\n        logger.info(\n            f\"  Effective batch size (w. parallel, distributed & accumulation) = {self.effective_batch_size}\")\n        logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n        logger.info(f\" Optimization steps = {max_steps} ({num_epochs} epochs)\")\n\n    def deploy(self):\n        # This is not included in __init__ to allow loading Trainer\n        self.model.to(self.device)\n\n        if self.fp16:\n            try:\n                from apex import amp\n            except ImportError:\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            self.model, optimizer = amp.initialize(\n                self.model, self.optimizer, opt_level=self.fp16_opt_level)\n            self.amp = amp\n\n        # multi-gpu training (should be after apex fp16 initialization)\n        if self.n_gpu > 1:\n            self.model = torch.nn.DataParallel(self.model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.local_rank != -1:\n            self.model = torch.nn.parallel.DistributedDataParallel(\n                self.model, device_ids=[self.local_rank], output_device=self.local_rank,\n                find_unused_parameters=True\n            )\n        self.deployed = True\n\n    @property\n    def n_samples(self):\n        return len(self.train_dataloader)\n\n    @property\n    def n_steps_per_epoch(self):\n        return len(self.train_dataloader) // self.gradient_accumulation_steps\n\n    @property\n    def current_epoch(self):\n        return self.global_step // self.n_steps_per_epoch\n\n    @property\n    def current_step(self):\n        return self.global_step % self.n_steps_per_epoch\n\n    @property\n    def is_top(self) -> bool:\n        # whether or not\n        return self.local_rank in [-1, 0]\n\n    @property\n    def effective_batch_size(self) -> int:\n        n_gpus = torch.distributed.get_world_size() if self.local_rank != -1 else 1\n        return self.per_gpu_train_batch_size * self.gradient_accumulation_steps * n_gpus\n\n    def train(self):\n        if not self.deployed:\n            raise RuntimeError('Trainer must be deployed before training.')\n\n        self.model.zero_grad()\n        pbar = tqdm(\n            total=int(self.max_steps), initial=self.global_step,\n            desc=f\"Train (epoch {self.current_epoch + 1})\", disable=not self.is_top,\n            dynamic_ncols=True\n        )\n        step = 0\n        \n        train_losses = []\n        dev_losses = []\n        \n        while (self.global_step + 1) <= self.max_steps:\n    \n            epoch_losses_train = []\n            epoch_loss_dev = 0\n    \n            if self.local_rank != -1:\n                self.train_dataloader.sampler.set_epoch(self.current_epoch)\n            pbar.set_description(desc=f\"Train (epoch {self.current_epoch + 1})\")\n    \n            for batch in self.train_dataloader:\n                # Skip past any already trained steps if resuming training\n                if (step // self.gradient_accumulation_steps) < self.current_step:\n                    step += 1\n                    continue\n\n                loss = self.run_batch(batch, train=True)\n        \n                if self.gradient_accumulation_steps > 1:\n                    loss = loss / self.gradient_accumulation_steps\n                \n                if math.isnan(loss.item()):\n#                     print(\"nan encounter while training\")\n                    pass\n                else:\n                    epoch_losses_train.append(loss.item())\n                \n                if self.fp16:\n                    with self.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n\n                if (step + 1) % self.gradient_accumulation_steps == 0:\n                    if self.max_grad_norm is not None:\n                        if self.fp16:\n                            torch.nn.utils.clip_grad_norm_(\n                                self.amp.master_params(self.optimizer), self.max_grad_norm)\n                        else:\n                            torch.nn.utils.clip_grad_norm_(\n                                self.model.parameters(), self.max_grad_norm)\n\n                    self.optimizer.step()\n                    self.scheduler.step()  # Update learning rate schedule\n                    self.model.zero_grad()\n                    if self.is_top:\n                        self.tb_writer.write(self.global_step)\n                    self.global_step += 1\n\n                    if self.is_top:\n                        pbar.update(n=1)\n\n\n                    if self.dev_dataloader is not None and self.global_step % self.valid_steps == 0:\n                        loss = self.evaluate()\n                        \n                        if loss < self.best_loss:\n                            self.best_loss = loss\n                            if self.is_top:\n                                self.save(self.best_checkpoint_dir)\n\n                    if self.is_top and self.save_steps > 0 and self.global_step % self.save_steps == 0:\n                        self.save()\n                step += 1\n                \n                if (self.global_step + 1) >= self.max_steps:\n                    break\n            epoch_loss_dev = self.evaluate()\n            if math.isnan(epoch_loss_dev):\n                print(\"nan encounter while validation check\")\n    \n            train_losses.append(np.mean(epoch_losses_train))\n            dev_losses.append(epoch_loss_dev)\n            print(\"Train_loss; \", np.mean(epoch_losses_train), \"Validation_loss: \", epoch_loss_dev)\n        \n\n        # Create the x-axis values from 1 to the length of the arrays\n        y1 = train_losses\n        x = range(1, len(y1) + 1)\n        y2 = dev_losses\n        # Plotting the data\n        plt.plot(x, y1, marker='o', linestyle='-', color='b', label='Train Loss')  # First line\n        plt.plot(x, y2, marker='x', linestyle='--', color='r', label='Dev Loss ')  # Second line\n\n        # Adding labels, title, and legend\n        plt.xlabel('X-axis (Index)')\n        plt.ylabel('Y-axis (Values)')\n        plt.title('Plot with Two Lines on Y-axis')\n        plt.legend()\n\n        # Show the plot\n        plt.show()\n\n        pbar.close()\n\n    def evaluate(self):\n        epoch_iterator = tqdm(\n            self.dev_dataloader, desc=\"Iteration (dev)\", disable=not self.is_top)\n        if self.is_top:\n            self.tb_writer.clear()\n        losses = []\n        for _, batch in enumerate(self.dev_dataloader):\n            loss = self.run_batch(batch, train=False)\n            \n            if math.isnan(loss.item()):\n                pass\n#                 print(\"nan encounter while evaluation\")\n            else:\n                losses.append(loss.item())\n    \n        if self.is_top:\n            self.tb_writer.write(self.global_step)\n#         print(\"Evaluate: \", losses)\n        return np.mean(losses)\n\n    def run_batch(self, batch, train: bool):\n        if train:\n            self.model.train()\n        else:\n            self.model.eval()\n        inputs = self.converter(batch, self.model, self.device)\n        outputs = self.model(**inputs)\n\n        loss, loss_cls = outputs.loss, outputs.loss_cls,\n        if self.task == 'identification_classification':\n            loss_span = outputs.loss_span\n\n        if self.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n            loss_cls = loss_cls.mean()\n            if self.task == 'identification_classification':\n                loss_span = loss_span.mean()\n\n        if self.is_top:\n            prefix = 'train' if train else 'eval'\n            self.tb_writer.add_scalar(f\"{prefix}/lr\", self.scheduler.get_last_lr()[0])\n            self.tb_writer.add_scalar(f\"{prefix}/loss\", loss.item())\n            self.tb_writer.add_scalar(f\"{prefix}/loss_cls\", loss_cls.item())\n            self.tb_writer.add_scalar(\n                f'{prefix}/accuracy_nli',\n                (np.argmax(outputs.class_logits.detach().cpu().numpy(), axis=1) == inputs['class_labels'].cpu().numpy()).mean())\n            if self.task == 'identification_classification':\n                self.tb_writer.add_scalar(f\"{prefix}/loss_span\", loss_span.item())\n                mask = inputs['p_mask'].cpu().numpy()\n                probs = scipy.special.softmax(outputs.span_logits.detach().cpu().numpy(), axis=2)[:, :, 1]\n                labels = inputs['span_labels'].cpu().numpy().copy()\n                labels[:, 0] = 1\n                if len(set(labels.flat[mask.flat == 0])) > 1:\n                    self.tb_writer.add_scalar(\n                        f'{prefix}/map_span',\n                        sklearn.metrics.average_precision_score(\n                            labels.flat[mask.flat == 0],\n                            probs.flat[mask.flat == 0]))\n        return loss\n\n    @property\n    def best_checkpoint_dir(self) -> str:\n        return os.path.join(self.output_dir, f\"best-checkpoint\")\n\n    def save(self, checkpoint_dir: Optional[str] = None):\n        if checkpoint_dir is None:\n            checkpoint_dir = os.path.join(\n                self.output_dir, f\"checkpoint-{self.global_step}\")\n        # Take care of distributed/parallel training\n        logger.info(\"Saving model checkpoint to %s\", checkpoint_dir)\n        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n        model_to_save.save_pretrained(checkpoint_dir)\n        logger.info(\"Saving optimizer and scheduler states to %s\", checkpoint_dir)\n        torch.save(self.optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n        torch.save(self.scheduler.state_dict(), os.path.join(checkpoint_dir, \"scheduler.pt\"))\n\n        with open(os.path.join(checkpoint_dir, 'trainer_info.json'), 'w') as fout:\n            json.dump({\n                'global_step': self.global_step,\n                'best_loss': self.best_loss,\n                'task': self.task\n            }, fout, indent=2)\n        logger.info(\"Finished saving Trainer.\")\n\n    def resume(self, output_dir: str):\n        checkpoint_dirs = glob.glob(os.path.join(output_dir, 'checkpoint-*'))\n        checkpoint_dir = max(checkpoint_dirs, key=lambda d: int(d.split(\"-\")[-1]))\n        self.load(checkpoint_dir)\n\n    def load(self, checkpoint_dir):\n        with open(os.path.join(checkpoint_dir, 'trainer_info.json')) as fin:\n            trainer_info = json.load(fin)\n        self.global_step = trainer_info['global_step']\n        if not os.path.exists(self.best_checkpoint_dir) and trainer_info['best_loss'] != np.inf:\n            logger.warning(\n                f'Previous \"best_loss\" was {trainer_info[\"best_loss\"]} but '\n                'the corresponding checkpoint was not found at '\n                f'{self.best_checkpoint_dir}. Resetting it to inf.')\n            self.best_loss = np.inf\n        else:\n            self.best_loss = trainer_info['best_loss']\n        self.task = trainer_info['task']\n        if self.task == 'identification_classification':\n            self.converter = identification_classification_converter\n        else:\n            self.converter = classification_converter\n\n        torch.cuda.empty_cache()\n        self.optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"optimizer.pt\")))\n        self.scheduler.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"scheduler.pt\")))\n\n        model_cls = type(self.model.module if hasattr(self.model, \"module\") else self.model)\n        self.model.to('cpu')\n        del self.model\n        torch.cuda.empty_cache()\n        self.model = model_cls.from_pretrained(checkpoint_dir)\n\n        self.deployed = False\n\n        logger.info(f'Loaded Trainer from {checkpoint_dir}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:46.880691Z","iopub.execute_input":"2024-11-18T22:13:46.881160Z","iopub.status.idle":"2024-11-18T22:13:46.940760Z","shell.execute_reply.started":"2024-11-18T22:13:46.881117Z","shell.execute_reply":"2024-11-18T22:13:46.939970Z"},"trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.\n# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/0c9bae09340dd8c6fdf6aa2ea5637e956efe0f7c/examples/question-answering/run_squad.py\n# See git log for changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\nimport contextlib\n\nimport numpy as np\nimport torch\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n@contextlib.contextmanager\ndef distributed_barrier(blocked: bool, enable: bool = True):\n    if enable and blocked:\n        torch.distributed.barrier()\n    yield\n    if enable and not blocked:\n        torch.distributed.barrier()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:47.051684Z","iopub.execute_input":"2024-11-18T22:13:47.052430Z","iopub.status.idle":"2024-11-18T22:13:47.058766Z","shell.execute_reply.started":"2024-11-18T22:13:47.052397Z","shell.execute_reply":"2024-11-18T22:13:47.057598Z"},"trusted":true},"outputs":[],"execution_count":59},{"cell_type":"code","source":"conf = {\n    'model_name_or_path': 'bert-base-uncased',\n    'train_file': '/kaggle/input/anlp-project-data/train.json',\n    'dev_file': '/kaggle/input/anlp-project-data/dev.json',\n    'config_name': None,\n    'tokenizer_name': None,\n    'cache_dir': None,\n    'max_seq_length': 512,\n    'doc_stride': 64,\n    'max_query_length': 256,\n    'do_lower_case': True,\n    'per_gpu_train_batch_size': 8,\n    'per_gpu_eval_batch_size': 8,\n    'learning_rate': 3e-5,\n    'gradient_accumulation_steps': 1,\n    'weight_decay': 0.0,\n    'adam_epsilon': 1e-8,\n    'max_grad_norm': 1.0,\n    'num_epochs': 5.0,\n    'max_steps': None,\n    'warmup_steps': 200,\n    'lang_id': None,\n    'valid_steps': 3000,\n    'early_stopping': True,\n    'save_steps': -1,\n    'seed': 42,\n    'fp16': False,\n    'fp16_opt_level': 'O1',\n    'no_cuda': False,\n    'overwrite_cache': False,\n    'weight_class_probs_by_span_probs': True,\n    'class_loss_weight': 0.1,\n    'task': 'identification_classification',\n    'symbol_based_hypothesis': False\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:47.270772Z","iopub.execute_input":"2024-11-18T22:13:47.271601Z","iopub.status.idle":"2024-11-18T22:13:47.277521Z","shell.execute_reply.started":"2024-11-18T22:13:47.271560Z","shell.execute_reply":"2024-11-18T22:13:47.276598Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.\n# Copyright (c) 2021, Hitachi America Ltd. All rights reserved.\n# This file has been adopted from https://github.com/huggingface/transformers\n# /blob/0c9bae09340dd8c6fdf6aa2ea5637e956efe0f7c/examples/question-answering/run.py\n# See git log for changes.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport json\nimport logging\nimport os\n\nimport click\nimport torch\nimport transformers\nfrom transformers import AutoConfig, AutoTokenizer\nfrom transformers.trainer_utils import is_main_process\n\n# from contract_nli.conf import load_conf\n# from contract_nli.dataset.dataset import load_and_cache_examples, load_and_cache_features\n# from contract_nli.dataset.encoder import SPAN_TOKEN\n# from contract_nli.evaluation import evaluate_all\n# from contract_nli.model.identification_classification import \\\n#     MODEL_TYPE_TO_CLASS, update_config\n# from contract_nli.model.classification import BertForClassification\n# from contract_nli.postprocess import format_json\n# from contract_nli.predictor import predict, predict_classification\n# from contract_nli.trainer import Trainer, setup_optimizer\n# from contract_nli.utils import set_seed, distributed_barrier\n\nlogger = logging.getLogger(__name__)\n\n\n# @click.command()\n# @click.argument('conf', type=click.Path(exists=True))\n# @click.argument('output-dir', type=click.Path(exists=False))\n# @click.option(\n#     '--local_rank', type=int, default=-1,\n#     help='This is automatically set by torch.distributed.launch.')\n# @click.option('--shared-filesystem', type=int, default=-1)\n# conf, output_dir, local_rank, shared_filesystem\noutput_dir = './output'\nlocal_rank = -1\nshared_filesystem = -1\n\n# Setup CUDA, GPU & distributed training\nif local_rank == -1 or conf['no_cuda']:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and not conf['no_cuda'] else \"cpu\")\n    n_gpu = 0 if conf['no_cuda'] else torch.cuda.device_count()\nelse:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n    torch.cuda.set_device(local_rank)\n    device = torch.device(\"cuda\", local_rank)\n    torch.distributed.init_process_group(backend=\"nccl\")\n    n_gpu = 1\n\n# if this is a main process in a node\nlocal_main = is_main_process(local_rank)\n# if this is a main process in the whole distributed training\nall_main = local_rank == -1 or torch.distributed.get_rank() == 0\n# if this is a main process on a filesystem\nfs_main = (shared_filesystem and all_main) or ((not shared_filesystem) and local_main)\n\n# Setup logging\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    filename = \"/kaggle/working/output.log\",\n    level=logging.INFO if local_main else logging.WARN,\n)\nlogger.warning(\n    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n    local_rank,\n    device,\n    n_gpu,\n    bool(local_rank != -1),\n    conf['fp16'],\n)\n# Set the verbosity to info of the Transformers logger (on main process only):\nif is_main_process(local_rank):\n    transformers.utils.logging.set_verbosity_info()\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n# Set seed\nset_seed(conf['seed'])\n\nwith distributed_barrier(not fs_main, local_rank != -1):\n    config = AutoConfig.from_pretrained(\n        conf['config_name'] if conf['config_name'] else conf['model_name_or_path'],\n        cache_dir=conf['cache_dir']\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        conf['tokenizer_name'] if conf['tokenizer_name'] else conf['model_name_or_path'],\n        do_lower_case=conf['do_lower_case'],\n        cache_dir=conf['cache_dir'],\n        use_fast=False\n    )\n    if conf['task'] == 'identification_classification':\n        config = update_config(\n            config, impossible_strategy='ignore',\n            class_loss_weight=conf['class_loss_weight'])\n        model = MODEL_TYPE_TO_CLASS[config.model_type].from_pretrained(\n            conf['model_name_or_path'],\n            from_tf=bool(\".ckpt\" in conf['model_name_or_path']),\n            config=config,\n            cache_dir=conf['cache_dir']\n        )\n    else:\n        model = BertForClassification.from_pretrained(\n            conf['model_name_or_path'],\n            from_tf=bool(\".ckpt\" in conf['model_name_or_path']),\n            config=config,\n            cache_dir=conf['cache_dir']\n        )\n\nlogger.info(\"Training/evaluation parameters %s\",\n            {k: v for k, v in conf.items() if k != 'raw_yaml'})\n\n# Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if conf['fp16'] is set.\n# Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n# remove the need for this code, but it is still valid.\nif conf['fp16']:\n    try:\n        import apex\n        apex.amp.register_half_function(torch, \"einsum\")\n    except ImportError:\n        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\nwith distributed_barrier(not fs_main, local_rank != -1):\n    examples = load_and_cache_examples(\n        conf['train_file'],\n        local_rank=local_rank,\n        overwrite_cache=conf['overwrite_cache'],\n        cache_dir='.',\n    )\n    if conf['task'] == 'identification_classification':\n        n_added_token = tokenizer.add_special_tokens(\n            {'additional_special_tokens': tokenizer.additional_special_tokens + [SPAN_TOKEN]})\n        if n_added_token == 0:\n            logger.warning(\n                f'SPAN_TOKEN \"{SPAN_TOKEN}\" was not added. You can safely ignore'\n                ' this warning if you are retraining a model from this train.py')\n        else:\n            span_token_id = tokenizer.additional_special_tokens_ids[\n                tokenizer.additional_special_tokens.index(SPAN_TOKEN)]\n            logger.warning(\n                f'SPAN_TOKEN \"{SPAN_TOKEN}\" was added as \"{span_token_id}\". You can safely ignore'\n                ' this warning if you are training a model from pretrained LMs.')\n    if conf['symbol_based_hypothesis']:\n        hypothesis_symbols = sorted(set([e.hypothesis_symbol for e in examples]))\n        n_added_token = tokenizer.add_special_tokens(\n            {'additional_special_tokens': tokenizer.additional_special_tokens + hypothesis_symbols})\n        if n_added_token == 0:\n            logger.warning(\n                f'SPAN_TOKEN \"{SPAN_TOKEN}\" was not added. You can safely ignore'\n                ' this warning if you are retraining a model from this train.py')\n        else:\n            assert n_added_token == len(hypothesis_symbols)\n            hypothesis_symbol_dic = {\n                s: tokenizer.additional_special_tokens_ids[tokenizer.additional_special_tokens.index(s)]\n                for s in hypothesis_symbols\n            }\n            logger.warning(\n                f'Hypothesis symbols were added as \"{hypothesis_symbol_dic}\". '\n                'You can safely ignore this warning if you are training a '\n                'model from pretrained LMs.')\n    model.resize_token_embeddings(len(tokenizer))\n\n    train_dataset = load_and_cache_features(\n        conf['train_file'],\n        examples,\n        tokenizer,\n        max_seq_length=conf['max_seq_length'],\n        doc_stride=conf.get('doc_stride', None),\n        max_query_length=conf['max_query_length'],\n        dataset_type=conf['task'],\n        symbol_based_hypothesis=conf['symbol_based_hypothesis'],\n        threads=None,\n        local_rank=local_rank,\n        overwrite_cache=conf['overwrite_cache'],\n        labels_available=True,\n        cache_dir='.',\n    )[0]\n\nif conf['dev_file'] is not None:\n    with distributed_barrier(not fs_main, local_rank != -1):\n        dev_examples = load_and_cache_examples(\n            conf['dev_file'],\n            local_rank=local_rank,\n            overwrite_cache=conf['overwrite_cache'],\n            cache_dir='.'\n        )\n        dev_dataset, dev_features = load_and_cache_features(\n            conf['dev_file'],\n            dev_examples,\n            tokenizer,\n            max_seq_length=conf['max_seq_length'],\n            doc_stride=conf.get('doc_stride', None),\n            max_query_length=conf['max_query_length'],\n            dataset_type=conf['task'],\n            symbol_based_hypothesis=conf['symbol_based_hypothesis'],\n            threads=None,\n            local_rank=local_rank,\n            overwrite_cache=conf['overwrite_cache'],\n            labels_available=True,\n            cache_dir='.'\n        )\n\nelse:\n    dev_dataset, dev_examples, dev_features = None, None, None\n\noptimizer = setup_optimizer(\n    model, learning_rate=conf['learning_rate'], epsilon=conf['adam_epsilon'],\n    weight_decay=conf['weight_decay'])\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    optimizer=optimizer,\n    task=conf['task'],\n    output_dir=output_dir,\n    per_gpu_train_batch_size=conf['per_gpu_train_batch_size'],\n    num_epochs=conf['num_epochs'],\n    max_steps=conf['max_steps'],\n    dev_dataset=dev_dataset,\n    valid_steps=conf['valid_steps'],\n    per_gpu_dev_batch_size=conf['per_gpu_eval_batch_size'],\n    gradient_accumulation_steps=conf['gradient_accumulation_steps'],\n    warmup_steps=conf['warmup_steps'],\n    max_grad_norm=conf['max_grad_norm'],\n    n_gpu=n_gpu,\n    local_rank=local_rank,\n    fp16=conf['fp16'],\n    fp16_opt_level=conf['fp16_opt_level'],\n    device=device,\n    save_steps=conf['save_steps'])\ntrainer.deploy()\ntrainer.train()\n\n# FIXME: Prediction using multiple GPUs\n# if not all_main:\n#     return\n\n# Setup CUDA, GPU & distributed training\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:47.495395Z","iopub.execute_input":"2024-11-18T22:13:47.495723Z","iopub.status.idle":"2024-11-19T02:22:07.833337Z","shell.execute_reply.started":"2024-11-18T22:13:47.495693Z","shell.execute_reply":"2024-11-19T02:22:07.832256Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[INFO|configuration_utils.py:659] 2024-11-18 22:13:47,652 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n[INFO|configuration_utils.py:708] 2024-11-18 22:13:47,654 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|configuration_utils.py:659] 2024-11-18 22:13:47,909 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n[INFO|configuration_utils.py:708] 2024-11-18 22:13:47,910 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|tokenization_utils_base.py:1781] 2024-11-18 22:13:48,620 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n[INFO|tokenization_utils_base.py:1781] 2024-11-18 22:13:48,622 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1781] 2024-11-18 22:13:48,622 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1781] 2024-11-18 22:13:48,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n[INFO|configuration_utils.py:659] 2024-11-18 22:13:48,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n[INFO|configuration_utils.py:708] 2024-11-18 22:13:48,761 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|modeling_utils.py:2107] 2024-11-18 22:13:48,919 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n[WARNING|modeling_utils.py:2474] 2024-11-18 22:13:50,468 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForIdentificationClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForIdentificationClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForIdentificationClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:2486] 2024-11-18 22:13:50,469 >> Some weights of BertForIdentificationClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['span_outputs.bias', 'class_outputs.weight', 'class_outputs.bias', 'span_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTrain (epoch 1):   4%|▎         | 187/5130 [03:38<1:36:25,  1.17s/it]\n100%|██████████| 423/423 [00:38<00:00, 11.09it/s]\n[INFO|tokenization_utils_base.py:888] 2024-11-18 22:15:16,872 >> Assigning ['[SPAN]'] to the additional_special_tokens key of the tokenizer\n[INFO|tokenization_utils.py:426] 2024-11-18 22:15:16,873 >> Adding [SPAN] to the vocabulary\nconvert examples to features: 100%|██████████| 7191/7191 [09:16<00:00, 12.91it/s]\nadd example index and unique id: 100%|██████████| 7191/7191 [00:00<00:00, 205701.85it/s]\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:436: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:207.)\n100%|██████████| 61/61 [00:05<00:00, 10.28it/s]\nconvert examples to features: 100%|██████████| 1037/1037 [01:31<00:00, 11.36it/s]\nadd example index and unique id: 100%|██████████| 1037/1037 [00:00<00:00, 176135.63it/s]\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\nTrain (epoch 1):  11%|█         | 3000/27425 [22:08<3:00:01,  2.26it/s]\nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\n[INFO|configuration_utils.py:446] 2024-11-18 22:52:17,332 >> Configuration saved in ./output/best-checkpoint/config.json\n[INFO|modeling_utils.py:1660] 2024-11-18 22:52:18,519 >> Model weights saved in ./output/best-checkpoint/pytorch_model.bin\nTrain (epoch 1):  20%|██        | 5485/27425 [42:38<2:28:07,  2.47it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 2):  20%|██        | 5485/27425 [44:45<2:28:07,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Train_loss;  0.08381621883108492 Validation_loss:  0.0631235157346589\n","output_type":"stream"},{"name":"stderr","text":"Train (epoch 2):  22%|██▏       | 6000/27425 [48:33<2:38:16,  2.26it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\n[INFO|configuration_utils.py:446] 2024-11-18 23:18:42,190 >> Configuration saved in ./output/best-checkpoint/config.json\n[INFO|modeling_utils.py:1660] 2024-11-18 23:18:43,440 >> Model weights saved in ./output/best-checkpoint/pytorch_model.bin\nTrain (epoch 2):  33%|███▎      | 9000/27425 [1:12:51<2:16:08,  2.26it/s]\nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\n[INFO|configuration_utils.py:446] 2024-11-18 23:43:00,711 >> Configuration saved in ./output/best-checkpoint/config.json\n[INFO|modeling_utils.py:1660] 2024-11-18 23:43:01,912 >> Model weights saved in ./output/best-checkpoint/pytorch_model.bin\nTrain (epoch 2):  40%|████      | 10970/27425 [1:29:34<1:51:05,  2.47it/s] \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 3):  40%|████      | 10970/27425 [1:31:40<1:51:05,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Train_loss;  0.046331909895195776 Validation_loss:  0.05252902121031405\n","output_type":"stream"},{"name":"stderr","text":"Train (epoch 3):  44%|████▍     | 12000/27425 [1:39:17<1:53:47,  2.26it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 3):  55%|█████▍    | 15000/27425 [2:03:31<1:31:34,  2.26it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 3):  60%|██████    | 16455/27425 [2:16:22<1:14:11,  2.46it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 4):  60%|██████    | 16455/27425 [2:18:28<1:14:11,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Train_loss;  0.033312445076071895 Validation_loss:  0.06056238915766037\n","output_type":"stream"},{"name":"stderr","text":"Train (epoch 4):  66%|██████▌   | 18000/27425 [2:29:52<1:09:37,  2.26it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 4):  77%|███████▋  | 21000/27425 [2:54:06<47:24,  2.26it/s]    \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 4):  80%|████████  | 21940/27425 [3:03:08<37:01,  2.47it/s]   \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 5):  80%|████████  | 21940/27425 [3:05:14<37:01,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Train_loss;  0.022356375094323797 Validation_loss:  0.06383423201903468\n","output_type":"stream"},{"name":"stderr","text":"Train (epoch 5):  88%|████████▊ | 24000/27425 [3:20:25<25:13,  2.26it/s]   \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 5):  98%|█████████▊| 27000/27425 [3:44:38<03:09,  2.24it/s]   \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 5): 100%|█████████▉| 27424/27425 [3:49:52<00:00,  2.26it/s]  \nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\nTrain (epoch 5): 100%|█████████▉| 27424/27425 [3:51:58<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"Train_loss;  0.01295955339464501 Validation_loss:  0.0753662080640736\n","output_type":"stream"},{"name":"stderr","text":"Train (epoch 5): 100%|██████████| 27425/27425 [3:51:58<00:00, 38.28s/it]\nIteration (dev):   0%|          | 0/894 [02:06<?, ?it/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Train_loss;  0.0005838444922119379 Validation_loss:  0.07578861797235405\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"name":"stderr","text":"Train (epoch 5): 100%|██████████| 27425/27425 [3:54:05<00:00,  1.95it/s]\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"if local_rank != -1:\n    trainer.local_rank = -1\n\nif conf['early_stopping'] and os.path.exists(trainer.best_checkpoint_dir):\n    logger.info(f\"Loading best model from {trainer.best_checkpoint_dir}\")\n    trainer.load(trainer.best_checkpoint_dir)\n    trainer.deploy()\n    model = trainer.model\n\nlogger.info(\"Saving model checkpoint to %s\", output_dir)\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\n# Take care of distributed/parallel training\nmodel_to_save = model.module if hasattr(model, \"module\") else model\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n#     with open(os.path.join(output_dir, \"conf.yml\"), 'w') as fout:\n#         fout.write(conf['raw_yaml'])\n\nif dev_dataset is not None:\n    logger.info(\"Evaluate the on validation data\")\n    if conf['task'] == 'identification_classification':\n        all_results = predict(\n            model, dev_dataset, dev_examples, dev_features,\n            per_gpu_batch_size=conf['per_gpu_eval_batch_size'],\n            device=device, n_gpu=n_gpu,\n            weight_class_probs_by_span_probs=conf['weight_class_probs_by_span_probs'])\n    else:\n        all_results = predict_classification(\n            model, dev_dataset, dev_features,\n            per_gpu_batch_size=conf['per_gpu_eval_batch_size'],\n            device=device, n_gpu=n_gpu)\n    result_json = format_json(dev_examples, all_results)\n    with open(os.path.join(output_dir, f'result.json'), 'w') as fout:\n        json.dump(result_json, fout, indent=2)\n    with open(conf['dev_file']) as fin:\n        dev_json = json.load(fin)\n    \n    dev_json['documents'] = dev_json['documents'][: len(dev_json['documents']) // 5]\n    \n    metrics = evaluate_all(dev_json, result_json,\n                           [1, 3, 5, 8, 10, 15, 20, 30, 40, 50],\n                           conf['task'])\n    logger.info(f\"Results@: {json.dumps(metrics, indent=2)}\")\n    with open(os.path.join(output_dir, f'metrics.json'), 'w') as fout:\n        json.dump(metrics, fout, indent=2)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T02:22:07.836241Z","iopub.execute_input":"2024-11-19T02:22:07.836635Z","iopub.status.idle":"2024-11-19T02:24:48.093748Z","shell.execute_reply.started":"2024-11-19T02:22:07.836606Z","shell.execute_reply":"2024-11-19T02:24:48.092980Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[INFO|configuration_utils.py:657] 2024-11-19 02:22:09,490 >> loading configuration file ./output/best-checkpoint/config.json\n[INFO|configuration_utils.py:708] 2024-11-19 02:22:09,492 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForIdentificationClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"class_loss_weight\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"impossible_strategy\": \"ignore\",\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30523\n}\n\n[INFO|modeling_utils.py:2105] 2024-11-19 02:22:09,493 >> loading weights file ./output/best-checkpoint/pytorch_model.bin\n[INFO|modeling_utils.py:2483] 2024-11-19 02:22:10,875 >> All model checkpoint weights were used when initializing BertForIdentificationClassification.\n\n[INFO|modeling_utils.py:2492] 2024-11-19 02:22:10,876 >> All the weights of BertForIdentificationClassification were initialized from the model checkpoint at ./output/best-checkpoint.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForIdentificationClassification for predictions without further training.\n[INFO|configuration_utils.py:446] 2024-11-19 02:22:11,002 >> Configuration saved in ./output/config.json\n[INFO|modeling_utils.py:1660] 2024-11-19 02:22:12,225 >> Model weights saved in ./output/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2123] 2024-11-19 02:22:12,228 >> tokenizer config file saved in ./output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2130] 2024-11-19 02:22:12,230 >> Special tokens file saved in ./output/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2175] 2024-11-19 02:22:12,231 >> added tokens file saved in ./output/added_tokens.json\nEvaluating: 100%|██████████| 894/894 [02:19<00:00,  6.40it/s]\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T02:24:48.094795Z","iopub.execute_input":"2024-11-19T02:24:48.095064Z","iopub.status.idle":"2024-11-19T02:24:49.941100Z","shell.execute_reply.started":"2024-11-19T02:24:48.095039Z","shell.execute_reply":"2024-11-19T02:24:49.939936Z"},"trusted":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import shutil\n\nfolder_path = '/kaggle/working/output'  # Replace with your folder path\nzip_file_path = '/kaggle/working/output.zip'\n\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n\n\nfrom IPython.display import FileLink\n\nFileLink('/kaggle/working/output.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:59:42.988087Z","iopub.execute_input":"2024-11-12T20:59:42.988399Z","iopub.status.idle":"2024-11-12T21:01:13.460113Z","shell.execute_reply.started":"2024-11-12T20:59:42.988369Z","shell.execute_reply":"2024-11-12T21:01:13.459061Z"},"trusted":true},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output.zip","text/html":"<a href='/kaggle/working/output.zip' target='_blank'>/kaggle/working/output.zip</a><br>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"# # import os\n# os.remove(\"/kaggle/working/cached_examples_dev\")\n# os.remove(\"/kaggle/working/cached_examples_train\")\n# os.remove(\"/kaggle/working/cached_features_dev_identification_classification_bert-base-uncased_512_256_64\")\n# os.remove(\"/kaggle/working/cached_features_train_identification_classification_bert-base-uncased_512_256_64\")","metadata":{"execution":{"iopub.status.busy":"2024-11-18T22:13:04.747650Z","iopub.execute_input":"2024-11-18T22:13:04.748031Z","iopub.status.idle":"2024-11-18T22:13:04.890363Z","shell.execute_reply.started":"2024-11-18T22:13:04.748001Z","shell.execute_reply":"2024-11-18T22:13:04.889594Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}