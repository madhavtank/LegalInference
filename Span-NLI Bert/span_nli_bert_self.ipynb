{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9566977,"sourceType":"datasetVersion","datasetId":5830902}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nfrom transformers import AutoTokenizer, PreTrainedModel, PretrainedConfig, AutoModel, Trainer, TrainingArguments, EarlyStoppingCallback","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:45.847016Z","iopub.execute_input":"2024-11-10T19:43:45.847778Z","iopub.status.idle":"2024-11-10T19:43:45.852929Z","shell.execute_reply.started":"2024-11-10T19:43:45.847735Z","shell.execute_reply":"2024-11-10T19:43:45.851968Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:45.855279Z","iopub.execute_input":"2024-11-10T19:43:45.855821Z","iopub.status.idle":"2024-11-10T19:43:45.864595Z","shell.execute_reply.started":"2024-11-10T19:43:45.855757Z","shell.execute_reply":"2024-11-10T19:43:45.863718Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def clean_str(str_):\n    str_ = str_.replace('\\n', ' ')\n    str_ = re.sub(r'\\\\t', ' ', str_)\n    str_ = re.sub(r'\\\\r', ' ', str_)\n    str_ = re.sub(r'(.)\\1{2,}', r'\\1', str_)\n    return str_.strip().lower()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:45.865741Z","iopub.execute_input":"2024-11-10T19:43:45.866122Z","iopub.status.idle":"2024-11-10T19:43:45.874919Z","shell.execute_reply.started":"2024-11-10T19:43:45.866091Z","shell.execute_reply":"2024-11-10T19:43:45.874094Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:45.876913Z","iopub.execute_input":"2024-11-10T19:43:45.877240Z","iopub.status.idle":"2024-11-10T19:43:46.136189Z","shell.execute_reply.started":"2024-11-10T19:43:45.877208Z","shell.execute_reply":"2024-11-10T19:43:46.135197Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_path = '/kaggle/input/anlp-project-data/train.json'\ndev_path = '/kaggle/input/anlp-project-data/dev.json'\ntest_path = '/kaggle/input/anlp-project-data/test.json'\n\nwith open(train_path, 'r') as train_file:\n    train_data = json.load(train_file)\n\nwith open(dev_path, 'r') as dev_file:\n    dev_data = json.load(dev_file)\n\nhypothesis = {}\nfor key, value in train_data['labels'].items():\n    hypothesis[key] = clean_str(value['hypothesis'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-10T19:43:46.137308Z","iopub.execute_input":"2024-11-10T19:43:46.137609Z","iopub.status.idle":"2024-11-10T19:43:46.216176Z","shell.execute_reply.started":"2024-11-10T19:43:46.137576Z","shell.execute_reply":"2024-11-10T19:43:46.215196Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"train_data = train_data['documents']\ndev_data = dev_data['documents']\n\ntrain_data = train_data[:len(train_data) // 10]\ndev_data = dev_data[:len(dev_data) // 10]","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:46.217375Z","iopub.execute_input":"2024-11-10T19:43:46.217675Z","iopub.status.idle":"2024-11-10T19:43:46.226377Z","shell.execute_reply.started":"2024-11-10T19:43:46.217644Z","shell.execute_reply":"2024-11-10T19:43:46.225350Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:46.227832Z","iopub.execute_input":"2024-11-10T19:43:46.228250Z","iopub.status.idle":"2024-11-10T19:43:46.397962Z","shell.execute_reply.started":"2024-11-10T19:43:46.228205Z","shell.execute_reply":"2024-11-10T19:43:46.397199Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"label_dict = {\n    'NotMentioned': 0,\n    'Entailment': 1,\n    'Contradiction': 2,\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:46.400564Z","iopub.execute_input":"2024-11-10T19:43:46.400973Z","iopub.status.idle":"2024-11-10T19:43:46.405535Z","shell.execute_reply.started":"2024-11-10T19:43:46.400930Z","shell.execute_reply":"2024-11-10T19:43:46.404559Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def get_hypothesis_idx(hypothesis_name):\n    return int(hypothesis_name.split('-')[-1])\n\nclass NLIDataset(Dataset):\n    def __init__(self, documents, tokenizer, hypothesis, context_sizes, surround_character_size):\n#         label_dict = label_dict\n        self.tokenizer = tokenizer\n\n        self.tokenizer.add_special_tokens({'additional_special_tokens': ['[SPAN]']})\n\n        data_points = []\n        contexts = [{}]\n\n        for context_size in context_sizes:\n            for i, doc in enumerate(documents):\n                char_idx = 0\n                while char_idx < len(doc['text']):\n                    document_spans = doc['spans']\n                    cur_context = {\n                        'doc_id': i,\n                        'start_char_idx': char_idx,\n                        'end_char_idx': char_idx + context_size,\n                        'spans' : [],\n                    }\n\n                    for j, (start, end) in enumerate(document_spans):\n                        if end <= char_idx:\n                            continue\n\n                        cur_context['spans'].append({\n                            'start_char_idx': max(start, char_idx),\n                            'end_char_idx': min(end, char_idx + context_size),\n                            'marked': start >= char_idx and end <= char_idx + context_size,\n                            'span_id': j\n                        })\n\n                        if end > char_idx + context_size:\n                            break\n\n                    if cur_context == contexts[-1]:\n                        char_idx = cur_context['end_char_idx'] - surround_character_size\n                        continue\n\n                    contexts.append(cur_context)\n                    if len(cur_context['spans']) == 1 and not cur_context['spans'][0]['marked']:\n                        char_idx = cur_context['end_char_idx'] - surround_character_size\n                    else:\n                        char_idx = cur_context['spans'][-1]['start_char_idx'] - surround_character_size\n\n        contexts.pop(0)\n\n        for nda_name, nda_desc in hypothesis.items():\n            for i, context in enumerate(contexts):\n\n                nli_label = label_dict[documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['choice']]\n\n                data_point = {}\n                data_point['hypotheis'] = nda_desc\n                cur_premise = \"\"\n                data_point['marked_beg'] = context['spans'][0]['marked']\n                data_point['marked_end'] = context['spans'][-1]['marked']\n                doc_id = context['doc_id']\n                hypothesis_id = get_hypothesis_idx(nda_name)\n                span_ids = []\n\n                if len(context['spans']) == 1:\n                    data_point['marked_end'] = True\n\n                span_labels = []\n\n                for span in context['spans']:\n                    val = int(span['span_id'] in documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['spans'])\n\n                    val = 2 * val - 1 # making 0 -> -1 and 1 -> 1\n\n                    if span['marked']:\n                        span_labels.append(val)\n                        span_ids.append(span['span_id'])\n\n                    cur_premise += ' [SPAN] '\n                    cur_premise += documents[context['doc_id']]['text'][span['start_char_idx']:span['end_char_idx']]\n\n                data_point['premise'] = cur_premise\n                \n                if nli_label == label_dict['NotMentioned']:\n                    span_labels = torch.zeros(len(span_labels), dtype=torch.long)\n\n                data_point['nli_label'] = torch.tensor(nli_label, dtype=torch.long)\n                data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n                data_point['doc_id'] = torch.tensor(doc_id, dtype=torch.long)\n                data_point['hypothesis_id'] = torch.tensor(hypothesis_id, dtype=torch.long)\n                data_point['span_ids'] = torch.tensor(span_ids, dtype=torch.long)\n\n                data_points.append(data_point)\n\n        self.data_points = data_points\n        self.span_token_id = self.tokenizer.convert_tokens_to_ids('[SPAN]')\n\n    def __len__(self):\n        return len(self.data_points)\n\n    def __getitem__(self, idx):\n        tokenized_data = self.tokenizer(\n            [self.data_points[idx]['hypotheis']],\n            [self.data_points[idx]['premise']],\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n        )\n\n        tokenized_data['input_ids'] = tokenized_data['input_ids'].squeeze()\n        tokenized_data['attention_mask'] = tokenized_data['attention_mask'].squeeze()\n        tokenized_data['token_type_ids'] = tokenized_data['token_type_ids'].squeeze()\n\n        span_indices = torch.where(tokenized_data['input_ids'] == self.span_token_id)[0]\n\n        if not self.data_points[idx]['marked_beg']:\n            span_indices = span_indices[1:]\n        \n        if not self.data_points[idx]['marked_end'] or tokenized_data['attention_mask'][-1] == 0:\n            span_indices = span_indices[:-1]\n        \n        span_ids = self.data_points[idx]['span_ids']\n        span_ids = span_ids[:len(span_indices)]\n\n        return {\n            'input_ids': tokenized_data['input_ids'],\n            'attention_mask': tokenized_data['attention_mask'],\n            'token_type_ids': tokenized_data['token_type_ids'],\n            'span_indices': span_indices,\n            'nli_label': self.data_points[idx]['nli_label'],\n            'span_labels': self.data_points[idx]['span_labels'][:len(span_indices)],\n            'data_for_metrics': {\n                'doc_id': self.data_points[idx]['doc_id'],\n                'hypothesis_id': self.data_points[idx]['hypothesis_id'],\n                'span_ids': span_ids,\n            }\n        }","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:46.409140Z","iopub.execute_input":"2024-11-10T19:43:46.409550Z","iopub.status.idle":"2024-11-10T19:43:46.434857Z","shell.execute_reply.started":"2024-11-10T19:43:46.409513Z","shell.execute_reply":"2024-11-10T19:43:46.434028Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"train_dataset = NLIDataset(train_data, tokenizer, hypothesis, [1000, 1100, 1200], 50)\ndev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1000, 1100, 1200], 50)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:46.436137Z","iopub.execute_input":"2024-11-10T19:43:46.436904Z","iopub.status.idle":"2024-11-10T19:43:48.552368Z","shell.execute_reply.started":"2024-11-10T19:43:46.436865Z","shell.execute_reply":"2024-11-10T19:43:48.551604Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2968097133.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_class_weights(dataset):\n    nli_labels = [x['nli_label'] for x in dataset]\n\n    span_labels = []\n    for x in dataset:\n        span_labels.extend(x['span_labels'].tolist())\n\n    nli_weights = compute_class_weight('balanced', classes=np.unique(nli_labels), y=np.array(nli_labels))\n\n    nli_weights = nli_weights.tolist()\n\n    span_labels = [x for x in span_labels if x != -1]\n    span_labels = np.array(span_labels)\n    span_weight = np.sum(span_labels == 0) / np.sum(span_labels)\n\n    return nli_weights, span_weight\n\nnli_weights, span_weight = get_class_weights(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:43:48.553692Z","iopub.execute_input":"2024-11-10T19:43:48.554028Z","iopub.status.idle":"2024-11-10T19:45:11.325964Z","shell.execute_reply.started":"2024-11-10T19:43:48.553993Z","shell.execute_reply":"2024-11-10T19:45:11.325122Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class ContractNLIConfig(PretrainedConfig):\n    def __init__(self, nli_weights = [1, 1, 1], span_weight = 1, lambda_ = 1, bert_model_name = model_name, num_labels = len(label_dict), ignore_span_label = 2, **kwargs):\n        super().__init__(**kwargs)\n        self.bert_model_name = bert_model_name\n        self.num_labels = num_labels\n        self.lambda_ = lambda_\n        self.ignore_span_label = ignore_span_label\n        self.nli_weights = nli_weights\n        self.span_weight = span_weight","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:11.327087Z","iopub.execute_input":"2024-11-10T19:45:11.327398Z","iopub.status.idle":"2024-11-10T19:45:11.333240Z","shell.execute_reply.started":"2024-11-10T19:45:11.327365Z","shell.execute_reply":"2024-11-10T19:45:11.332392Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class ContractNLI(PreTrainedModel):\n    config_class = ContractNLIConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = AutoModel.from_pretrained(config.bert_model_name)\n        self.bert.resize_token_embeddings(self.bert.config.vocab_size + 1, pad_to_multiple_of=8)\n        self.bert.eval()\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n        self.embedding_dim = self.bert.config.hidden_size\n        self.num_labels = config.num_labels\n        self.lambda_ = config.lambda_\n\n#         ic(self.config.nli_weights)\n#         ic(self.config.span_weight)\n\n        self.nli_criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.config.nli_weights, dtype=torch.float32))\n        self.span_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.config.span_weight, dtype=torch.float32))\n\n        self.span_classifier = nn.Sequential(\n            nn.Linear(self.embedding_dim, self.embedding_dim * 4),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 2, 1)\n        )\n\n        self.nli_classifier = nn.Sequential(\n            nn.Linear(self.embedding_dim, self.embedding_dim * 4),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim * 2, self.num_labels)\n        )\n\n        # initialize weights\n        self.init_weights()\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # use the same initialization as bert\n            module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, input_ids, attention_mask, token_type_ids, span_indices):\n        outputs = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True).hidden_states[-1]\n\n        gather = torch.gather(outputs, 1, span_indices.unsqueeze(2).expand(-1, -1, outputs.shape[-1]))\n\n        masked_gather = gather[span_indices != 0]\n        span_logits = self.span_classifier(masked_gather)\n        nli_logits = self.nli_classifier(outputs[:, 0, :])\n\n        return span_logits, nli_logits","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:11.334524Z","iopub.execute_input":"2024-11-10T19:45:11.334832Z","iopub.status.idle":"2024-11-10T19:45:11.350837Z","shell.execute_reply.started":"2024-11-10T19:45:11.334779Z","shell.execute_reply":"2024-11-10T19:45:11.349963Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"class ContractNLITrainer(Trainer):\n    def __init__(self, *args, data_collator=None, **kwargs):\n        super().__init__(*args, data_collator=data_collator, **kwargs)\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        span_label = inputs.pop('span_labels')\n        nli_label = inputs.pop('nli_label')\n        inputs.pop('data_for_metrics')\n\n        outputs = model(**inputs)\n        span_logits, nli_logits = outputs[0], outputs[1]\n        \n        # span labels = -1, means ignore \n        \n        mask = span_label != -1\n        span_label = span_label[mask]\n        span_logits = span_logits[mask]\n        \n        span_label = span_label.float()\n        span_logits = span_logits.float()\n        \n        span_label = span_label.view(-1)\n        span_logits = span_logits.view(-1)        \n\n        if len(span_label) == 0:\n            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n        else:\n            span_loss = self.model.span_criterion(span_logits, span_label)\n\n        nli_loss = self.model.nli_criterion(nli_logits, nli_label)\n\n        if torch.isnan(nli_loss):\n            nli_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n\n        if torch.isnan(span_loss):\n            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n\n        loss = span_loss + self.model.lambda_ * nli_loss\n\n        if loss.item() == 0:\n            loss = torch.tensor(0, dtype=torch.float32, device=DEVICE, requires_grad=True)\n\n        return (loss, outputs) if return_outputs else loss\n\n    @staticmethod\n    def collate_fn(features):\n        span_indices_list = [feature['span_indices'] for feature in features]\n        max_len = max([len(span_indices) for span_indices in span_indices_list])\n        span_indices_list = [torch.cat([span_indices, torch.zeros(max_len - len(span_indices), dtype=torch.long)]) for span_indices in span_indices_list]\n\n        span_ids_list = [feature['data_for_metrics']['span_ids'] for feature in features]\n        max_len = max([len(span_ids) for span_ids in span_ids_list])\n        \n        span_ids_list = [torch.cat([span_ids, torch.full((max_len - len(span_ids),), -1)]) for span_ids in span_ids_list]\n        \n        input_ids = torch.stack([feature['input_ids'] for feature in features])\n        attention_mask = torch.stack([feature['attention_mask'] for feature in features])\n        token_type_ids = torch.stack([feature['token_type_ids'] for feature in features])\n        span_indices = torch.stack(span_indices_list)\n        nli_label = torch.stack([feature['nli_label'] for feature in features])\n        span_label = torch.cat([feature['span_labels'] for feature in features], dim=0)\n        data_for_metrics = {\n            'doc_id': torch.stack([feature['data_for_metrics']['doc_id'] for feature in features]),\n            'hypothesis_id': torch.stack([feature['data_for_metrics']['hypothesis_id'] for feature in features]),\n            'span_ids': torch.stack(span_ids_list),\n        }\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids,\n            'span_indices': span_indices,\n            'nli_label': nli_label,\n            'span_labels': span_label,\n            'data_for_metrics': data_for_metrics,\n        }","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:11.351956Z","iopub.execute_input":"2024-11-10T19:45:11.352245Z","iopub.status.idle":"2024-11-10T19:45:11.371598Z","shell.execute_reply.started":"2024-11-10T19:45:11.352215Z","shell.execute_reply":"2024-11-10T19:45:11.370692Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    auto_find_batch_size=True,\n    output_dir='/kaggle/working/',   # output directory\n    num_train_epochs=20,            # total number of training epochs\n    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n    logging_strategy='epoch',\n    # eval_steps=1000,\n    # save_steps=1000,\n    # logging_steps=1000,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    fp16=True,\n    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n    report_to='none',\n#     report_to='wandb',\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:11.372660Z","iopub.execute_input":"2024-11-10T19:45:11.372960Z","iopub.status.idle":"2024-11-10T19:45:11.415106Z","shell.execute_reply.started":"2024-11-10T19:45:11.372929Z","shell.execute_reply":"2024-11-10T19:45:11.414354Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def model_init(trial):\n    if trial is None:\n        return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight))\n\n    return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight, lambda_=trial['lambda_']))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:11.416121Z","iopub.execute_input":"2024-11-10T19:45:11.416409Z","iopub.status.idle":"2024-11-10T19:45:11.421151Z","shell.execute_reply.started":"2024-11-10T19:45:11.416379Z","shell.execute_reply":"2024-11-10T19:45:11.420272Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"trainer = ContractNLITrainer(\n    model=None,                          # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=dev_dataset,            # evaluation dataset\n    data_collator=ContractNLITrainer.collate_fn,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n    model_init=model_init,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:11.422181Z","iopub.execute_input":"2024-11-10T19:45:11.422485Z","iopub.status.idle":"2024-11-10T19:45:12.401250Z","shell.execute_reply.started":"2024-11-10T19:45:11.422440Z","shell.execute_reply":"2024-11-10T19:45:12.400472Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T19:45:12.402340Z","iopub.execute_input":"2024-11-10T19:45:12.402657Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='624' max='7740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 624/7740 17:05 < 3:15:37, 0.61 it/s, Epoch 1.61/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.242900</td>\n      <td>8.275131</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}