{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9566627,"sourceType":"datasetVersion","datasetId":5830640},{"sourceId":9850442,"sourceType":"datasetVersion","datasetId":6044228}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install sentence_transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:33:34.246021Z","iopub.execute_input":"2024-11-20T16:33:34.246352Z","iopub.status.idle":"2024-11-20T16:33:43.808583Z","shell.execute_reply.started":"2024-11-20T16:33:34.246323Z","shell.execute_reply":"2024-11-20T16:33:43.807492Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch -y\n!pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:33:43.810440Z","iopub.execute_input":"2024-11-20T16:33:43.810730Z","iopub.status.idle":"2024-11-20T16:38:25.579233Z","shell.execute_reply.started":"2024-11-20T16:33:43.810703Z","shell.execute_reply":"2024-11-20T16:38:25.578139Z"}},"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - pytorch\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: failed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - package cuda-version-12.3-h55a0123_2 has constraint cudatoolkit 12.3|12.3.* conflicting with cudatoolkit-10.0.130-0\n\nCould not solve for environment specs\nThe following packages are incompatible\n├─ \u001b[32mcuda-version 12.3** \u001b[0m is installable and it requires\n│  └─ \u001b[32mcudatoolkit 12.3|12.3.* \u001b[0m, which can be installed;\n└─ \u001b[31mcudatoolkit 10.0** \u001b[0m is not installable because it conflicts with any installable versions previously reported.\n\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load training data\nwith open(\"/kaggle/input/dataset-contractnli/train.json\", \"r\") as f:\n    train_data = json.load(f)\n\nwith open(\"/kaggle/input/dataset-contractnli/test.json\", \"r\") as f:\n    test_data = json.load(f)\n\nwith open(\"/kaggle/input/contractnli-dev/dev.json\", \"r\") as f:\n    val_data = json.load(f)\n\n# Load BERT models\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nnli_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n\n# Load SentenceTransformer for semantic search\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:38:25.580793Z","iopub.execute_input":"2024-11-20T16:38:25.581156Z","iopub.status.idle":"2024-11-20T16:38:56.659562Z","shell.execute_reply.started":"2024-11-20T16:38:25.581117Z","shell.execute_reply":"2024-11-20T16:38:56.658707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343de659c4dc49a792f3748a60db7175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442ac047d20d4729b8ed5e5e29a6b1f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387f074328d64bf4b59f957c7f5b7fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bb494528264515a70223b721c854c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57094470844c4fa9aa8630df2216bed4"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0affa18ba1724cf098b19b0811b81a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ae36dc304b43e98df39992b52713a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5ced0f80fa40b6a33fc96569834b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f1a4ea908f4f70bb9970ec3e1a2f80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f57727905cce4d4c8b1cb2823fdb3712"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94024d51671d41eb8c7487a2eb7dc155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d610ada0244ce6bb9bc9f6cd024e36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8ae1fce864419bbf2841cd068e7fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c6a2f62d2ac486ebd06cea5c742822c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bbbbaf0799c4f0f835eb46b6107a7f1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c17a3f9941542d4b2c35da01b74baa4"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:38:56.662072Z","iopub.execute_input":"2024-11-20T16:38:56.663108Z","iopub.status.idle":"2024-11-20T16:38:56.666649Z","shell.execute_reply.started":"2024-11-20T16:38:56.663063Z","shell.execute_reply":"2024-11-20T16:38:56.665802Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def prepare_document_embeddings(documents):\n    \"\"\"Embed all sentences in each document for semantic search.\"\"\"\n    document_embeddings = []\n    for doc in documents:\n        sentences = doc.split('.')  # Adjust sentence splitting as needed\n        embeddings = embedder.encode(sentences, convert_to_tensor=True,show_progress_bar=False)\n        document_embeddings.append((sentences, embeddings))\n    return document_embeddings\n\n# Extract documents for embedding preparation\n# train_data['documents'] = train_data['documents'][:5]\ndocuments = [item['text'] for item in train_data['documents']]\ndocument_embeddings = prepare_document_embeddings(documents)\n\ndocuments = [item['text'] for item in val_data['documents']]\nval_embeddings = prepare_document_embeddings(documents)\n\ndocuments = [item['text'] for item in test_data['documents']]\ntest_embeddings = prepare_document_embeddings(documents)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:38:56.667681Z","iopub.execute_input":"2024-11-20T16:38:56.667938Z","iopub.status.idle":"2024-11-20T16:39:27.753033Z","shell.execute_reply.started":"2024-11-20T16:38:56.667914Z","shell.execute_reply":"2024-11-20T16:39:27.752288Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"label_dict = {'Entailment': 0, 'NotMentioned': 1, 'Contradiction': 2}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:39:27.754129Z","iopub.execute_input":"2024-11-20T16:39:27.754407Z","iopub.status.idle":"2024-11-20T16:39:27.758872Z","shell.execute_reply.started":"2024-11-20T16:39:27.754370Z","shell.execute_reply":"2024-11-20T16:39:27.757799Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to evaluate loss or accuracy\ndef evaluate(model, documents, labels, device,document_embeddings, mode=\"loss\" ):\n    total = 0\n    correct = 0\n    total_loss = 0\n    model.eval()\n    \n    with torch.no_grad():\n        cnt=0\n        for idx, doc in enumerate(documents):\n            doc_text = clean_text(doc['text'])\n            annotations = doc['annotation_sets'][0]['annotations']\n            document = document_embeddings[idx]  # Precomputed embeddings for the current document\n\n            for hypothesis_name, annotation in annotations.items():\n                choice = annotation['choice']\n                hypothesis = clean_text(labels[hypothesis_name]['hypothesis'])\n                label = label_dict[choice]\n                \n                # Retrieve top relevant sentence\n                hypothesis_embedding = embedder.encode(hypothesis, convert_to_tensor=True, show_progress_bar=False).to(device)\n                sentences, embeddings = document\n                cos_scores = util.pytorch_cos_sim(hypothesis_embedding, embeddings.to(device))[0]\n                top_result_idx = torch.argmax(cos_scores).item()\n                top_sentence = sentences[top_result_idx]\n                \n                # Prepare input for BERT: \"hypothesis <sep> result\"\n                input_text = f\"{hypothesis} [SEP] {top_sentence}\"\n                inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                inputs['labels'] = torch.tensor([label], dtype=torch.long).to(device)\n\n                \n                # Forward pass\n                outputs = model(**inputs)\n                \n                # print(input_text)\n                # print(outputs )\n                cnt+=1\n                if mode == \"loss\":\n                    loss = outputs.loss\n                    total_loss += loss.item()\n                elif mode == \"accuracy\":\n                    predictions = torch.argmax(outputs.logits, dim=-1)\n                    if predictions.item() == label:\n                        correct += 1\n                    total += 1\n\n    if mode == \"loss\":\n        return total_loss / cnt\n    elif mode == \"accuracy\":\n        return correct / total if total > 0 else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:39:27.760125Z","iopub.execute_input":"2024-11-20T16:39:27.760465Z","iopub.status.idle":"2024-11-20T16:39:28.056542Z","shell.execute_reply.started":"2024-11-20T16:39:27.760428Z","shell.execute_reply":"2024-11-20T16:39:28.055717Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW\nimport re\nfrom torch.nn.functional import softmax\nfrom sentence_transformers import util\n\n# Set up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Optimizer\noptimizer = AdamW(nli_model.parameters(), lr=2e-5)\n\n# Move models to device\nnli_model = nli_model.to(device)\nembedder = embedder.to(device)\n\n# Prepare hypotheses dictionary\nhypothesis_dict = {key: val['hypothesis'] for key, val in train_data['labels'].items()}\n\n# Data and label dictionary\ndata = train_data\ndocuments = data['documents']\nval_documents = val_data['documents']\nval_labels = val_data['labels']\n\ntest_documents = test_data['documents']\ntest_labels = test_data['labels']\n\nlabels = data['labels']\n\n# Text cleaning function\ndef clean_text(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n# Training loop with validation loss\nnum_epochs = 2\ntop_k = 7\nfor epoch in range(num_epochs):\n    nli_model.train()\n    \n    total_loss = 0\n    cnt = 0\n    \n    for idx, doc in enumerate(documents):\n        doc_text = clean_text(doc['text'])\n        \n        annotations = doc['annotation_sets'][0]['annotations']\n        document = document_embeddings[idx]  # Precomputed embeddings for the current document\n\n        for hypothesis_name, annotation in annotations.items():\n            cnt += 1\n            choice = annotation['choice']\n            hypothesis = clean_text(labels[hypothesis_name]['hypothesis'])\n            label = label_dict[choice]\n\n            # Retrieve top 10 relevant sentences\n            hypothesis_embedding = embedder.encode(hypothesis, convert_to_tensor=True, show_progress_bar=False).to(device)\n            sentences, embeddings = document\n            cos_scores = util.pytorch_cos_sim(hypothesis_embedding, embeddings.to(device))[0]\n            top_k_results = torch.topk(cos_scores, k=top_k)\n            top_sentences = [sentences[idx] for idx in top_k_results.indices]\n            top_scores = top_k_results.values\n\n            # Prepare inputs for BERT and compute weighted loss\n            weighted_loss = 0\n            weight_sum = torch.sum(top_scores).item()\n            for i in range(top_k):\n                input_text = f\"{hypothesis} [SEP] {top_sentences[i]}\"\n                inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n                inputs['labels'] = torch.tensor([label], dtype=torch.long).to(device)\n                \n                # Move inputs to device\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n\n                # Zero gradients\n                optimizer.zero_grad()\n                \n                # Forward pass\n                outputs = nli_model(**inputs)\n                loss = outputs.loss\n                \n                # Weighted loss based on similarity score\n                weighted_loss += (top_scores[i] / weight_sum) * loss\n\n            total_loss += weighted_loss.item()\n            \n            # Backward pass and optimization step\n            weighted_loss.backward()\n            optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / cnt}\")\n\n    # Validation Loss Calculation\n    val_loss = evaluate(nli_model, val_documents, val_labels, device, val_embeddings, mode=\"loss\")\n    print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss}\")\n\n# Save fine-tuned model\nnli_model.save_pretrained(\"fine_tuned_nli_model\")\ntokenizer.save_pretrained(\"fine_tuned_nli_model\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T16:39:28.057910Z","iopub.execute_input":"2024-11-20T16:39:28.058339Z","iopub.status.idle":"2024-11-20T17:28:04.337130Z","shell.execute_reply.started":"2024-11-20T16:39:28.058233Z","shell.execute_reply":"2024-11-20T17:28:04.336282Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Training Loss: 0.7533996435647182\nEpoch 1, Validation Loss: 0.7919737055768388\nEpoch 2, Training Loss: 0.7293168594582806\nEpoch 2, Validation Loss: 0.7771420771239923\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('fine_tuned_nli_model/tokenizer_config.json',\n 'fine_tuned_nli_model/special_tokens_map.json',\n 'fine_tuned_nli_model/vocab.txt',\n 'fine_tuned_nli_model/added_tokens.json')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n\n# Define evaluation function to compute metrics\ndef evaluate_metrics(nli_model, documents, labels, device, embeddings, top_k=7):\n    nli_model.eval()\n    all_true_labels = []\n    all_pred_labels = []\n    \n    with torch.no_grad():\n        for idx, doc in enumerate(documents):\n            document = embeddings[idx]  # Precomputed embeddings for the current document\n            annotations = doc['annotation_sets'][0]['annotations']\n            \n            for hypothesis_name, annotation in annotations.items():\n                choice = annotation['choice']\n                true_label = label_dict[choice]\n                hypothesis = clean_text(labels[hypothesis_name]['hypothesis'])\n                \n                # Retrieve top-k relevant sentences\n                hypothesis_embedding = embedder.encode(hypothesis, convert_to_tensor=True, show_progress_bar=False).to(device)\n                sentences, sentence_embeddings = document\n                cos_scores = util.pytorch_cos_sim(hypothesis_embedding, sentence_embeddings.to(device))[0]\n                top_k_results = torch.topk(cos_scores, k=top_k)\n                top_sentences = [sentences[idx] for idx in top_k_results.indices]\n                top_scores = top_k_results.values\n\n                # Weighted average for label prediction\n                weighted_logits_sum = torch.zeros((1, nli_model.num_labels)).to(device)\n                for i in range(top_k):\n                    input_text = f\"{hypothesis} [SEP] {top_sentences[i]}\"\n                    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n                    \n                    # Forward pass\n                    outputs = nli_model(**inputs)\n                    logits = outputs.logits\n                    weighted_logits_sum += top_scores[i] * logits\n                \n                # Calculate prediction by taking the label with max probability\n                avg_logits = weighted_logits_sum / torch.sum(top_scores)\n                predicted_label = torch.argmax(softmax(avg_logits, dim=-1), dim=-1).item()\n                \n                # Append to true and predicted labels\n                all_true_labels.append(true_label)\n                all_pred_labels.append(predicted_label)\n\n    # Calculate metrics\n    accuracy = (torch.tensor(all_pred_labels) == torch.tensor(all_true_labels)).float().mean().item()\n    precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels, all_pred_labels, average='weighted')\n    conf_matrix = confusion_matrix(all_true_labels, all_pred_labels)\n\n    # Print results\n    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n\n    return accuracy, precision, recall, f1, conf_matrix\n\n# Evaluate metrics\ntop_k = 7\naccuracy, precision, recall, f1, conf_matrix = evaluate_metrics(\n    nli_model, test_documents, test_labels, device, test_embeddings, top_k=top_k\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:28:04.338384Z","iopub.execute_input":"2024-11-20T17:28:04.338800Z","iopub.status.idle":"2024-11-20T17:30:32.057418Z","shell.execute_reply.started":"2024-11-20T17:28:04.338738Z","shell.execute_reply":"2024-11-20T17:30:32.056431Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 61.41%\nPrecision: 0.6377\nRecall: 0.6141\nF1 Score: 0.5869\nConfusion Matrix:\n[[864  98   6]\n [542 332  29]\n [ 70  62  88]]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom torch.nn.functional import softmax\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sentence_transformers import SentenceTransformer, util\n\n\n\n# Evidence identification function\ndef identify_evidence(document, hypothesis, nli_model, tokenizer, embedder, device):\n    # Step 1: Split document into sentences\n    sentences = document.split('. ')  # Split sentences based on period followed by a space\n    \n    # Step 2: Perform NLI classification for the hypothesis and document\n    input_text = f\"{hypothesis} [SEP] {document}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    outputs = nli_model(**inputs)\n    logits = outputs.logits\n    probs = softmax(logits, dim=-1)\n    nli_class = torch.argmax(probs).item()\n    \n    class_names = [\"entailment\", \"neutral\", \"contradiction\"]\n    predicted_class = class_names[nli_class]\n\n    # Step 3: Find the sentence that supports the predicted class with the highest probability\n    hypothesis_embedding = embedder.encode(hypothesis, convert_to_tensor=True).to(device)\n    sentence_embeddings = embedder.encode(sentences, convert_to_tensor=True).to(device)\n    cos_scores = util.pytorch_cos_sim(hypothesis_embedding, sentence_embeddings).squeeze(0)\n    \n    max_score_idx = torch.argmax(cos_scores).item()\n    supporting_sentence = sentences[max_score_idx]\n\n    # Step 4: Return results\n    return {\n        \"predicted_class\": predicted_class,\n        \"supporting_sentence\": supporting_sentence,\n        \"supporting_score\": cos_scores[max_score_idx].item()\n    }\n\n# Example usage\ndocument = \"\"\"\nThe contract states that the payment must be made within 30 days of delivery.\nIf the payment is delayed, a penalty of 2% per month is applicable.\nThe client also agrees to cover all shipping costs.\n\"\"\"\nhypothesis = \"The client is responsible for shipping costs.\"\n\nresult = identify_evidence(document, hypothesis, nli_model, tokenizer, embedder, device)\n\n# Output the results\nprint(\"Predicted Class:\", result['predicted_class'])\nprint(\"Supporting Sentence:\", result['supporting_sentence'])\nprint(\"Supporting Score:\", result['supporting_score'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:31:33.755465Z","iopub.execute_input":"2024-11-20T17:31:33.756260Z","iopub.status.idle":"2024-11-20T17:31:33.828918Z","shell.execute_reply.started":"2024-11-20T17:31:33.756224Z","shell.execute_reply":"2024-11-20T17:31:33.828028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd47cebf1f8d48c2bbb3b76909681a9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8989b8b9e88f471dae038d978bbf0e17"}},"metadata":{}},{"name":"stdout","text":"Predicted Class: entailment\nSupporting Sentence: \nThe contract states that the payment must be made within 30 days of delivery.\nIf the payment is delayed, a penalty of 2% per month is applicable.\nThe client also agrees to cover all shipping costs.\n\nSupporting Score: 0.48570507764816284\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}